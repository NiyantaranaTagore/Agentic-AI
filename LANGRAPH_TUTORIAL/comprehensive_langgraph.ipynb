{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ Complete LangGraph Tutorial: From Basics to Advanced Applications\n",
        "\n",
        "## ğŸ“– Comprehensive Guide for Building Stateful AI Agents\n",
        "\n",
        "Welcome to the most comprehensive LangGraph tutorial! This notebook will take you from complete beginner to building sophisticated AI agents step by step.\n",
        "\n",
        "### ğŸ¯ What You'll Learn\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- âœ… Understand LangGraph's core concepts (Nodes, Edges, State)\n",
        "- âœ… Build your first simple chatbot\n",
        "- âœ… Add memory and persistence to conversations\n",
        "- âœ… Implement tool calling and external integrations\n",
        "- âœ… Create human-in-the-loop workflows\n",
        "- âœ… Build multi-agent systems\n",
        "- âœ… Handle complex state management\n",
        "- âœ… Deploy production-ready applications\n",
        "\n",
        "### ğŸ“š Based on Official LangGraph Documentation\n",
        "\n",
        "This tutorial follows the [official LangGraph documentation](https://langchain-ai.github.io/langgraph/) and incorporates best practices from the LangGraph team.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Table of Contents\n",
        "\n",
        "1. **[Setup & Installation](#setup)**\n",
        "2. **[Part 1: Understanding LangGraph Fundamentals](#part1)**\n",
        "3. **[Part 2: Building Your First Simple Agent](#part2)**\n",
        "4. **[Part 3: Adding Memory with Checkpointing](#part3)**\n",
        "5. **[Part 4: Tool Integration & External APIs](#part4)**\n",
        "6. **[Part 5: Human-in-the-Loop Workflows](#part5)**\n",
        "7. **[Part 6: Advanced State Management](#part6)**\n",
        "8. **[Part 7: Multi-Agent Systems](#part7)**\n",
        "9. **[Part 8: Real-World Use Cases](#part8)**\n",
        "10. **[Part 9: Production Deployment](#part9)**\n",
        "\n",
        "Let's begin this exciting journey! ğŸŒŸ\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ”§ Setup & Installation {#setup}\n",
        "\n",
        "Before we dive into LangGraph, let's set up our environment properly.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "1. **Python 3.8+** installed on your system\n",
        "2. **API Keys** for LLM providers (we'll use OpenAI in this tutorial)\n",
        "3. **Basic Python knowledge** - understanding of functions, classes, and dictionaries\n",
        "\n",
        "### What is LangGraph?\n",
        "\n",
        "**LangGraph** is a library for building **stateful, multi-actor applications** with LLMs. It extends LangChain with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a **cyclic** manner.\n",
        "\n",
        "Key features:\n",
        "- ğŸ”„ **Cyclic workflows** (not just linear chains)\n",
        "- ğŸ’¾ **Persistent state** across interactions\n",
        "- ğŸ¯ **Conditional routing** between different paths\n",
        "- ğŸ”§ **Human-in-the-loop** capabilities\n",
        "- ğŸ“Š **Built-in observability** with LangSmith\n",
        "\n",
        "Think of it as a way to build AI agents that can:\n",
        "- Remember previous conversations\n",
        "- Make decisions about what to do next\n",
        "- Use tools and external APIs\n",
        "- Involve humans when needed\n",
        "- Handle complex, multi-step workflows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install -q langgraph langsmith langchain-openai python-dotenv\n",
        "\n",
        "# Optional: for visualization\n",
        "!pip install -q matplotlib graphviz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸš€ Environment setup complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "from typing import Annotated, Dict, List, Any\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Set up environment variables\n",
        "def setup_environment():\n",
        "    \"\"\"Setup API keys for the tutorial\"\"\"\n",
        "    \n",
        "    # OpenAI API Key\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        openai_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "    \n",
        "    # Optional: LangSmith for observability (highly recommended)\n",
        "    if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
        "        print(\"LangSmith setup (optional but recommended for debugging):\")\n",
        "        langsmith_key = getpass.getpass(\"Enter your LangSmith API Key (or press Enter to skip): \")\n",
        "        if langsmith_key:\n",
        "            os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key\n",
        "            os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "            os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph-Tutorial\"\n",
        "            print(\"âœ… LangSmith tracing enabled!\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Skipping LangSmith setup\")\n",
        "    \n",
        "    print(\"ğŸš€ Environment setup complete!\")\n",
        "\n",
        "# Run setup\n",
        "setup_environment()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Part 1: Understanding LangGraph Fundamentals {#part1}\n",
        "\n",
        "Before we build anything, let's understand the core concepts that make LangGraph powerful.\n",
        "\n",
        "### ğŸ”‘ Core Concepts\n",
        "\n",
        "#### 1. **State** \n",
        "- The \"memory\" of your application\n",
        "- Shared data structure that persists across all steps\n",
        "- Can contain messages, variables, flags, or any data you need\n",
        "\n",
        "#### 2. **Nodes**\n",
        "- Individual functions or operations in your workflow\n",
        "- Each node receives the current state and returns updates\n",
        "- Think of them as \"workers\" that do specific tasks\n",
        "\n",
        "#### 3. **Edges** \n",
        "- Connections between nodes that define the flow\n",
        "- Can be simple (A â†’ B) or conditional (A â†’ B or C based on logic)\n",
        "\n",
        "#### 4. **Graph**\n",
        "- The complete workflow combining nodes and edges\n",
        "- Defines how your AI agent behaves and makes decisions\n",
        "\n",
        "### ğŸ¯ Simple Mental Model\n",
        "\n",
        "Think of LangGraph like a **flowchart for AI agents**:\n",
        "\n",
        "```\n",
        "[User Input] â†’ [AI Thinks] â†’ [Uses Tool?] \n",
        "                    â†“              â†“\n",
        "               [Respond]      [Call Tool] â†’ [AI Thinks] â†’ [Respond]\n",
        "```\n",
        "\n",
        "But unlike a simple flowchart, LangGraph can:\n",
        "- Remember everything that happened before\n",
        "- Loop back to previous steps\n",
        "- Involve humans in the decision-making\n",
        "- Handle complex, branching logic\n",
        "\n",
        "Let's see this in action! ğŸ‘‡\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¤– Part 2: Building Your First Simple Agent {#part2}\n",
        "\n",
        "Let's start with the simplest possible LangGraph application - a basic chatbot that can have a conversation.\n",
        "\n",
        "### Step 1: Define the State\n",
        "\n",
        "The state is like the \"memory\" of our agent. For a chatbot, we need to remember the conversation history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… State defined!\n",
            "Our state has one field: 'messages' that will store the conversation history\n",
            "The add_messages function ensures new messages are appended, not replaced\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Define our State - this is the \"memory\" of our agent\n",
        "class State(TypedDict):\n",
        "    # messages will store our conversation history\n",
        "    # add_messages is a special function that appends new messages instead of replacing them\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "print(\"âœ… State defined!\")\n",
        "print(\"Our state has one field: 'messages' that will store the conversation history\")\n",
        "print(\"The add_messages function ensures new messages are appended, not replaced\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create the Language Model\n",
        "\n",
        "Now we need an AI model to power our chatbot. We'll use OpenAI's GPT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Language model initialized!\n",
            "Model: gpt-3.5-turbo\n",
            "Test response: Hello there! How are you doing today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",  # You can also use \"gpt-4\" if you have access\n",
        "    temperature=0.7,        # Controls creativity (0 = deterministic, 1 = very creative)\n",
        ")\n",
        "\n",
        "print(\"âœ… Language model initialized!\")\n",
        "print(f\"Model: {llm.model_name}\")\n",
        "\n",
        "# Let's test it quickly\n",
        "test_response = llm.invoke(\"Say hello in a friendly way!\")\n",
        "print(f\"Test response: {test_response.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create the Chatbot Node\n",
        "\n",
        "A **node** is a function that:\n",
        "1. Takes the current state as input\n",
        "2. Does some work (like calling the LLM)\n",
        "3. Returns updates to the state\n",
        "\n",
        "Let's create our chatbot node:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Chatbot node created!\n",
            "This node will:\n",
            "1. Take the conversation history from state\n",
            "2. Send it to the LLM\n",
            "3. Return the LLM's response to be added to state\n"
          ]
        }
      ],
      "source": [
        "def chatbot_node(state: State) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    The main chatbot node that processes messages and generates responses.\n",
        "    \n",
        "    Args:\n",
        "        state: Current state containing conversation history\n",
        "        \n",
        "    Returns:\n",
        "        Dict with new messages to add to state\n",
        "    \"\"\"\n",
        "    # Get the conversation history from state\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Call the LLM with the conversation history\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # Return the new message to be added to state\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "print(\"âœ… Chatbot node created!\")\n",
        "print(\"This node will:\")\n",
        "print(\"1. Take the conversation history from state\")\n",
        "print(\"2. Send it to the LLM\")\n",
        "print(\"3. Return the LLM's response to be added to state\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Build the Graph\n",
        "\n",
        "Now we'll create the graph by:\n",
        "1. Creating a StateGraph\n",
        "2. Adding our chatbot node\n",
        "3. Defining the flow (edges)\n",
        "4. Compiling it into a runnable application\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Simple chatbot graph created!\n",
            "Flow: START â†’ chatbot â†’ END\n",
            "Ready to chat! ğŸ‰\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Create the graph builder\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Step 2: Add our chatbot node\n",
        "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "\n",
        "# Step 3: Define the flow\n",
        "# START -> chatbot -> END\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Step 4: Compile the graph\n",
        "simple_chatbot = graph_builder.compile()\n",
        "\n",
        "print(\"âœ… Simple chatbot graph created!\")\n",
        "print(\"Flow: START â†’ chatbot â†’ END\")\n",
        "print(\"Ready to chat! ğŸ‰\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Test Your First LangGraph Agent!\n",
        "\n",
        "Let's test our simple chatbot. We'll send it a message and see how it responds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Testing Simple Chatbot\n",
            "========================================\n",
            "ğŸ‘¤ Human: Hello! My name is Niyantarana Tagore. What's your name?\n",
            "ğŸ¤– AI: Hello Niyantarana Tagore! I'm an AI assistant and I don't have a personal name as I'm here to assist you. How can I help you today?\n",
            "\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# Test our simple chatbot\n",
        "def test_simple_chatbot():\n",
        "    print(\"ğŸ¤– Testing Simple Chatbot\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Create initial state with a user message\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"Hello! My name is Niyantarana Tagore. What's your name?\")]\n",
        "    }\n",
        "    \n",
        "    # Run the chatbot\n",
        "    result = simple_chatbot.invoke(initial_state)\n",
        "    \n",
        "    # Print the conversation\n",
        "    for i, message in enumerate(result[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    return result\n",
        "\n",
        "# Run the test\n",
        "first_result = test_simple_chatbot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_simple_chatbot(user_input):\n",
        "    print(\"ğŸ¤– Testing Simple Chatbot\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=user_input)]\n",
        "    }\n",
        "    \n",
        "    result = simple_chatbot.invoke(initial_state)\n",
        "    \n",
        "    for i, message in enumerate(result[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Testing Simple Chatbot\n",
            "========================================\n",
            "ğŸ‘¤ Human: What is the GATE cutoff for ECE?\n",
            "ğŸ¤– AI: The GATE cutoff for ECE (Electronics and Communication Engineering) can vary each year depending on various factors such as the difficulty of the exam, number of applicants, and the highest score obtained. In general, the cutoff for ECE is usually in the range of 25-35 for general category candidates. However, it is important to note that the cutoff can vary for different institutes and categories of candidates (SC/ST/OBC). It is recommended to check the official GATE website or contact the respective institutes for the most accurate and up-to-date information on cutoff scores.\n",
            "\n",
            "========================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is the GATE cutoff for ECE?', additional_kwargs={}, response_metadata={}, id='78f50480-00cc-456a-9d0b-35a816b2b681'),\n",
              "  AIMessage(content='The GATE cutoff for ECE (Electronics and Communication Engineering) can vary each year depending on various factors such as the difficulty of the exam, number of applicants, and the highest score obtained. In general, the cutoff for ECE is usually in the range of 25-35 for general category candidates. However, it is important to note that the cutoff can vary for different institutes and categories of candidates (SC/ST/OBC). It is recommended to check the official GATE website or contact the respective institutes for the most accurate and up-to-date information on cutoff scores.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 117, 'prompt_tokens': 17, 'total_tokens': 134, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bncss9pkUaanVsPCy2QRmXxrGc04f', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--97202456-ced7-47ed-9979-f2fa3660074a-0', usage_metadata={'input_tokens': 17, 'output_tokens': 117, 'total_tokens': 134, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_simple_chatbot(\"What is the GATE cutoff for ECE?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¤– Testing Simple Chatbot\n",
            "========================================\n",
            "ğŸ‘¤ Human: What are the Subjects in GATE ECE?\n",
            "ğŸ¤– AI: The subjects covered in the GATE ECE (Electronics and Communication Engineering) exam are as follows:\n",
            "1. Engineering Mathematics\n",
            "2. Networks, Signals, and Systems\n",
            "3. Electronic Devices\n",
            "4. Analog Circuits\n",
            "5. Digital Circuits\n",
            "6. Control Systems\n",
            "7. Communications\n",
            "8. Electromagnetics\n",
            "\n",
            "These subjects cover a wide range of topics related to electronics and communication engineering and are essential for a thorough understanding of the field.\n",
            "\n",
            "========================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What are the Subjects in GATE ECE?', additional_kwargs={}, response_metadata={}, id='c5033d8b-6d90-4f93-a8a1-d67ee4334663'),\n",
              "  AIMessage(content='The subjects covered in the GATE ECE (Electronics and Communication Engineering) exam are as follows:\\n1. Engineering Mathematics\\n2. Networks, Signals, and Systems\\n3. Electronic Devices\\n4. Analog Circuits\\n5. Digital Circuits\\n6. Control Systems\\n7. Communications\\n8. Electromagnetics\\n\\nThese subjects cover a wide range of topics related to electronics and communication engineering and are essential for a thorough understanding of the field.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 17, 'total_tokens': 110, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bncu9OW8uJKgjJPHQ7HWe30J5zDQF', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--5586ae80-7b45-4f75-aa4c-deaf7dbf8bcb-0', usage_metadata={'input_tokens': 17, 'output_tokens': 93, 'total_tokens': 110, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_simple_chatbot(\"What are the Subjects in GATE ECE?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ‰ Congratulations!\n",
        "\n",
        "You just built your first LangGraph agent! But there's a problem...\n",
        "\n",
        "**The Issue**: Our chatbot doesn't remember previous conversations. Each time we call it, it starts fresh.\n",
        "\n",
        "Let's test this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§ª Testing Memory Issue\n",
            "========================================\n",
            "ğŸ‘¤ Human: What was my name again?\n",
            "ğŸ¤– AI: I'm sorry, I am not able to remember your name as I am a digital assistant and do not have the capability to retain personal information.\n",
            "\n",
            "âŒ As you can see, the chatbot doesn't remember your name!\n",
            "This is because each call starts with a fresh state.\n",
            "In the next section, we'll fix this with **memory and checkpointing**! ğŸ§ \n"
          ]
        }
      ],
      "source": [
        "# Test memory issue - the chatbot won't remember the previous conversation\n",
        "print(\"ğŸ§ª Testing Memory Issue\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Second message - asking about the name mentioned earlier\n",
        "second_state = {\n",
        "    \"messages\": [HumanMessage(content=\"What was my name again?\")]\n",
        "}\n",
        "\n",
        "result2 = simple_chatbot.invoke(second_state)\n",
        "\n",
        "for message in result2[\"messages\"]:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"ğŸ¤– AI: {message.content}\")\n",
        "\n",
        "print(\"\\nâŒ As you can see, the chatbot doesn't remember your name!\")\n",
        "print(\"This is because each call starts with a fresh state.\")\n",
        "print(\"In the next section, we'll fix this with **memory and checkpointing**! ğŸ§ \")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§  Part 3: Adding Memory with Checkpointing {#part3}\n",
        "\n",
        "The power of LangGraph really shines when we add **persistent memory**. This allows our agent to:\n",
        "\n",
        "- Remember conversations across multiple interactions\n",
        "- Resume from where it left off\n",
        "- Handle long-running workflows\n",
        "- Support human-in-the-loop scenarios\n",
        "\n",
        "### What is Checkpointing?\n",
        "\n",
        "**Checkpointing** automatically saves the state after each step. When you invoke the graph again with the same `thread_id`, it loads the saved state and continues from there.\n",
        "\n",
        "Think of it like a video game save system! ğŸ®\n",
        "\n",
        "### Step 1: Set Up a Checkpointer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langgraph-checkpoint-sqlite in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (2.0.10)\n",
            "Requirement already satisfied: aiosqlite>=0.20 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (0.21.0)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.21 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (2.0.26)\n",
            "Requirement already satisfied: sqlite-vec>=0.1.6 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (0.1.6)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from aiosqlite>=0.20->langgraph-checkpoint-sqlite) (4.13.2)\n",
            "Requirement already satisfied: langchain-core>=0.2.38 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.3.66)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.10.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.11.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.23.0)\n",
            "Requirement already satisfied: anyio in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (4.9.0)\n",
            "Requirement already satisfied: certifi in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph-checkpoint-sqlite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Checkpointer created!\n",
            "This will automatically save and restore conversation state\n",
            "In production, you'd use a real database file instead of ':memory:'\n"
          ]
        }
      ],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "print(\"âœ… Checkpointer created!\")\n",
        "print(\"This will automatically save and restore conversation state\")\n",
        "print(\"In production, you'd use a real database file instead of ':memory:'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create a Chatbot with Memory\n",
        "\n",
        "Now let's rebuild our chatbot with memory capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_builder_with_memory = StateGraph(State)\n",
        "\n",
        "graph_builder_with_memory.add_node(\"chatbot\", chatbot_node)\n",
        "graph_builder_with_memory.add_edge(START, \"chatbot\")\n",
        "graph_builder_with_memory.add_edge(\"chatbot\", END)\n",
        "\n",
        "chatbot_with_memory = graph_builder_with_memory.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID for this conversation: b8b16daf-62a6-4f62-a13a-b977e234c437\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "unique_id = uuid.uuid4()\n",
        "str(unique_id)\n",
        "print(f\"Unique ID for this conversation: {unique_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_id = \"b8b16daf-62a6-4f62-a13a-b977e234c437\"\n",
        "def test_simple(user_input):\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\":str(unique_id)}}\n",
        "    result = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=user_input)]},\n",
        "        config\n",
        "    )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Test the Memory\n",
        "\n",
        "The key to using memory is the **config** parameter with a `thread_id`. All conversations with the same `thread_id` will share memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ§  Testing Chatbot with Memory\n",
            "==================================================\n",
            "ğŸ“ First interaction:\n",
            "ğŸ‘¤ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "ğŸ¤– AI: Hello Niyantarana Tagore! It's nice to meet you. Can you tell me more about your work as an AI Agent Developer?\n",
            "ğŸ‘¤ Human: What's my name and what I do?\n",
            "ğŸ¤– AI: Your name is Niyantarana Tagore and you are an AI Agent Developer. You specialize in developing artificial intelligence agents to perform tasks and interact with users in various applications. Your work involves designing, programming, and testing AI algorithms to improve the performance and capabilities of these agents.\n",
            "ğŸ‘¤ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "ğŸ¤– AI: Hello Niyantarana Tagore! It's nice to meet you again. How can I assist you today?\n",
            "\n",
            "ğŸ“ Second interaction (same thread_id):\n",
            "ğŸ‘¤ Human: What's my name and what I do?\n",
            "ğŸ¤– AI: Your name is Niyantarana Tagore and you are an AI Agent Developer. You specialize in creating artificial intelligence agents to perform tasks and interact with users in various applications. Your work involves designing, programming, and testing AI algorithms to enhance the performance and capabilities of these agents. Is there anything specific you would like to discuss or inquire about today?\n",
            "\n",
            "âœ… Success! The chatbot remembered both your name and Specialization!\n"
          ]
        }
      ],
      "source": [
        "def test_memory_chatbot():\n",
        "    print(\"ğŸ§  Testing Chatbot with Memory\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Configuration with thread_id - this is crucial for memory!\n",
        "    config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "    \n",
        "    # First message\n",
        "    print(\"ğŸ“ First interaction:\")\n",
        "    result1 = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\")]},\n",
        "        config  # Pass config as second parameter!\n",
        "    )\n",
        "    \n",
        "    for message in result1[\"messages\"]:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\nğŸ“ Second interaction (same thread_id):\")\n",
        "    result2 = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"What's my name and what I do?\")]},\n",
        "        config  # Same config = same memory!\n",
        "    )\n",
        "    \n",
        "    # Only show the new messages\n",
        "    new_messages = result2[\"messages\"][len(result1[\"messages\"]):]\n",
        "    for message in new_messages:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ğŸ¤– AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\nâœ… Success! The chatbot remembered both your name and Specialization!\")\n",
        "    return result2\n",
        "\n",
        "# Test the memory\n",
        "memory_result = test_memory_chatbot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ” Understanding Thread IDs\n",
        "\n",
        "Different `thread_id`s create separate conversations. Let's test this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”„ Testing Different Thread ID\n",
            "========================================\n",
            "ğŸ‘¤ Human: What's my name?\n",
            "ğŸ¤– AI: I'm sorry, I do not have access to that information.\n",
            "\n",
            "ğŸ¯ Key Insight:\n",
            "- Same thread_id = shared memory\n",
            "- Different thread_id = separate conversations\n",
            "- This allows multiple users or conversation contexts!\n"
          ]
        }
      ],
      "source": [
        "# Test with a different thread_id\n",
        "print(\"ğŸ”„ Testing Different Thread ID\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Different thread_id = fresh conversation\n",
        "different_config = {\"configurable\": {\"thread_id\": \"conversation_2\"}}\n",
        "\n",
        "result_different = chatbot_with_memory.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What's my name?\")]},\n",
        "    different_config  # Different thread_id!\n",
        ")\n",
        "\n",
        "for message in result_different[\"messages\"]:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"ğŸ‘¤ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"ğŸ¤– AI: {message.content}\")\n",
        "\n",
        "print(\"\\nğŸ¯ Key Insight:\")\n",
        "print(\"- Same thread_id = shared memory\")\n",
        "print(\"- Different thread_id = separate conversations\")\n",
        "print(\"- This allows multiple users or conversation contexts!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ“Š Inspecting the State\n",
        "\n",
        "You can inspect the current state of any conversation using `get_state()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“Š State Inspection\n",
            "==============================\n",
            "Number of messages: 8\n",
            "Next node to execute: ()\n",
            "Thread ID: conversation_1\n",
            "\n",
            "ğŸ’¬ Full conversation history:\n",
            "1. ğŸ‘¤ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "2. ğŸ¤– AI: Hello Niyantarana Tagore! It's nice to meet you. Can you tell me more about your work as an AI Agent...\n",
            "3. ğŸ‘¤ Human: What's my name and what I do?\n",
            "4. ğŸ¤– AI: Your name is Niyantarana Tagore and you are an AI Agent Developer. You specialize in developing arti...\n",
            "5. ğŸ‘¤ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "6. ğŸ¤– AI: Hello Niyantarana Tagore! It's nice to meet you again. How can I assist you today?...\n",
            "7. ğŸ‘¤ Human: What's my name and what I do?\n",
            "8. ğŸ¤– AI: Your name is Niyantarana Tagore and you are an AI Agent Developer. You specialize in creating artifi...\n"
          ]
        }
      ],
      "source": [
        "# Inspect the state of our first conversation\n",
        "config1 = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "state_snapshot = chatbot_with_memory.get_state(config1)\n",
        "\n",
        "print(\"ğŸ“Š State Inspection\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Number of messages: {len(state_snapshot.values['messages'])}\")\n",
        "print(f\"Next node to execute: {state_snapshot.next}\")\n",
        "print(f\"Thread ID: {state_snapshot.config['configurable']['thread_id']}\")\n",
        "\n",
        "print(\"\\nğŸ’¬ Full conversation history:\")\n",
        "for i, message in enumerate(state_snapshot.values[\"messages\"], 1):\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"{i}. ğŸ‘¤ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"{i}. ğŸ¤– AI: {message.content[:100]}...\")  # Truncate for readability\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bia_genai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
