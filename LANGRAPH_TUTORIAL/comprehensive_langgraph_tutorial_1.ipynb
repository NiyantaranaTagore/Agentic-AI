{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Complete LangGraph Tutorial: From Basics to Advanced Applications\n",
        "\n",
        "## üìñ Comprehensive Guide for Building Stateful AI Agents\n",
        "\n",
        "Welcome to the most comprehensive LangGraph tutorial! This notebook will take you from complete beginner to building sophisticated AI agents step by step.\n",
        "\n",
        "### üéØ What You'll Learn\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- ‚úÖ Understand LangGraph's core concepts (Nodes, Edges, State)\n",
        "- ‚úÖ Build your first simple chatbot\n",
        "- ‚úÖ Add memory and persistence to conversations\n",
        "- ‚úÖ Implement tool calling and external integrations\n",
        "- ‚úÖ Create human-in-the-loop workflows\n",
        "- ‚úÖ Build multi-agent systems\n",
        "- ‚úÖ Handle complex state management\n",
        "- ‚úÖ Deploy production-ready applications\n",
        "\n",
        "### üìö Based on Official LangGraph Documentation\n",
        "\n",
        "This tutorial follows the [official LangGraph documentation](https://langchain-ai.github.io/langgraph/) and incorporates best practices from the LangGraph team.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. **[Setup & Installation](#setup)**\n",
        "2. **[Part 1: Understanding LangGraph Fundamentals](#part1)**\n",
        "3. **[Part 2: Building Your First Simple Agent](#part2)**\n",
        "4. **[Part 3: Adding Memory with Checkpointing](#part3)**\n",
        "5. **[Part 4: Tool Integration & External APIs](#part4)**\n",
        "6. **[Part 5: Human-in-the-Loop Workflows](#part5)**\n",
        "7. **[Part 6: Advanced State Management](#part6)**\n",
        "8. **[Part 7: Multi-Agent Systems](#part7)**\n",
        "9. **[Part 8: Real-World Use Cases](#part8)**\n",
        "10. **[Part 9: Production Deployment](#part9)**\n",
        "\n",
        "Let's begin this exciting journey! üåü\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup & Installation {#setup}\n",
        "\n",
        "Before we dive into LangGraph, let's set up our environment properly.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "1. **Python 3.8+** installed on your system\n",
        "2. **API Keys** for LLM providers (we'll use OpenAI in this tutorial)\n",
        "3. **Basic Python knowledge** - understanding of functions, classes, and dictionaries\n",
        "\n",
        "### What is LangGraph?\n",
        "\n",
        "**LangGraph** is a library for building **stateful, multi-actor applications** with LLMs. It extends LangChain with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a **cyclic** manner.\n",
        "\n",
        "Key features:\n",
        "- üîÑ **Cyclic workflows** (not just linear chains)\n",
        "- üíæ **Persistent state** across interactions\n",
        "- üéØ **Conditional routing** between different paths\n",
        "- üîß **Human-in-the-loop** capabilities\n",
        "- üìä **Built-in observability** with LangSmith\n",
        "\n",
        "Think of it as a way to build AI agents that can:\n",
        "- Remember previous conversations\n",
        "- Make decisions about what to do next\n",
        "- Use tools and external APIs\n",
        "- Involve humans when needed\n",
        "- Handle complex, multi-step workflows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q langgraph langsmith langchain-openai python-dotenv\n",
        "\n",
        "# Optional: for visualization\n",
        "!pip install -q matplotlib graphviz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Environment setup complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "from typing import Annotated, Dict, List, Any\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "# Set up environment variables\n",
        "def setup_environment():\n",
        "    \"\"\"Setup API keys for the tutorial\"\"\"\n",
        "    \n",
        "    # OpenAI API Key\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        openai_key = getpass.getpass(\"Enter your OpenAI API Key: \")\n",
        "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "    \n",
        "    # Optional: LangSmith for observability (highly recommended)\n",
        "    if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
        "        print(\"LangSmith setup (optional but recommended for debugging):\")\n",
        "        langsmith_key = getpass.getpass(\"Enter your LangSmith API Key (or press Enter to skip): \")\n",
        "        if langsmith_key:\n",
        "            os.environ[\"LANGSMITH_API_KEY\"] = langsmith_key\n",
        "            os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "            os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph-Tutorial\"\n",
        "            print(\"‚úÖ LangSmith tracing enabled!\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Skipping LangSmith setup\")\n",
        "    \n",
        "    print(\"üöÄ Environment setup complete!\")\n",
        "\n",
        "# Run setup\n",
        "setup_environment()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Part 1: Understanding LangGraph Fundamentals {#part1}\n",
        "\n",
        "Before we build anything, let's understand the core concepts that make LangGraph powerful.\n",
        "\n",
        "### üîë Core Concepts\n",
        "\n",
        "#### 1. **State** \n",
        "- The \"memory\" of your application\n",
        "- Shared data structure that persists across all steps\n",
        "- Can contain messages, variables, flags, or any data you need\n",
        "\n",
        "#### 2. **Nodes**\n",
        "- Individual functions or operations in your workflow\n",
        "- Each node receives the current state and returns updates\n",
        "- Think of them as \"workers\" that do specific tasks\n",
        "\n",
        "#### 3. **Edges** \n",
        "- Connections between nodes that define the flow\n",
        "- Can be simple (A ‚Üí B) or conditional (A ‚Üí B or C based on logic)\n",
        "\n",
        "#### 4. **Graph**\n",
        "- The complete workflow combining nodes and edges\n",
        "- Defines how your AI agent behaves and makes decisions\n",
        "\n",
        "### üéØ Simple Mental Model\n",
        "\n",
        "Think of LangGraph like a **flowchart for AI agents**:\n",
        "\n",
        "```\n",
        "[User Input] ‚Üí [AI Thinks] ‚Üí [Uses Tool?] \n",
        "                    ‚Üì              ‚Üì\n",
        "               [Respond]      [Call Tool] ‚Üí [AI Thinks] ‚Üí [Respond]\n",
        "```\n",
        "\n",
        "But unlike a simple flowchart, LangGraph can:\n",
        "- Remember everything that happened before\n",
        "- Loop back to previous steps\n",
        "- Involve humans in the decision-making\n",
        "- Handle complex, branching logic\n",
        "\n",
        "Let's see this in action! üëá\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Part 2: Building Your First Simple Agent {#part2}\n",
        "\n",
        "Let's start with the simplest possible LangGraph application - a basic chatbot that can have a conversation.\n",
        "\n",
        "### Step 1: Define the State\n",
        "\n",
        "The state is like the \"memory\" of our agent. For a chatbot, we need to remember the conversation history.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ State defined!\n",
            "Our state has one field: 'messages' that will store the conversation history\n",
            "The add_messages function ensures new messages are appended, not replaced\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Define our State - this is the \"memory\" of our agent\n",
        "class State(TypedDict):\n",
        "    # messages will store our conversation history\n",
        "    # add_messages is a special function that appends new messages instead of replacing them\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "print(\"‚úÖ State defined!\")\n",
        "print(\"Our state has one field: 'messages' that will store the conversation history\")\n",
        "print(\"The add_messages function ensures new messages are appended, not replaced\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create the Language Model\n",
        "\n",
        "Now we need an AI model to power our chatbot. We'll use OpenAI's GPT model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Language model initialized!\n",
            "Model: gpt-3.5-turbo\n",
            "Test response: Hello there! How are you doing today?\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Initialize the language model\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o\",  # You can also use \"gpt-4\" if you have access\n",
        "    temperature=0.7,        # Controls creativity (0 = deterministic, 1 = very creative)\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Language model initialized!\")\n",
        "print(f\"Model: {llm.model_name}\")\n",
        "\n",
        "# Let's test it quickly\n",
        "test_response = llm.invoke(\"Say hello in a friendly way!\")\n",
        "print(f\"Test response: {test_response.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Create the Chatbot Node\n",
        "\n",
        "A **node** is a function that:\n",
        "1. Takes the current state as input\n",
        "2. Does some work (like calling the LLM)\n",
        "3. Returns updates to the state\n",
        "\n",
        "Let's create our chatbot node:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Chatbot node created!\n",
            "This node will:\n",
            "1. Take the conversation history from state\n",
            "2. Send it to the LLM\n",
            "3. Return the LLM's response to be added to state\n"
          ]
        }
      ],
      "source": [
        "def chatbot_node(state: State) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    The main chatbot node that processes messages and generates responses.\n",
        "    \n",
        "    Args:\n",
        "        state: Current state containing conversation history\n",
        "        \n",
        "    Returns:\n",
        "        Dict with new messages to add to state\n",
        "    \"\"\"\n",
        "    # Get the conversation history from state\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Call the LLM with the conversation history\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # Return the new message to be added to state\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "print(\"‚úÖ Chatbot node created!\")\n",
        "print(\"This node will:\")\n",
        "print(\"1. Take the conversation history from state\")\n",
        "print(\"2. Send it to the LLM\")\n",
        "print(\"3. Return the LLM's response to be added to state\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Build the Graph\n",
        "\n",
        "Now we'll create the graph by:\n",
        "1. Creating a StateGraph\n",
        "2. Adding our chatbot node\n",
        "3. Defining the flow (edges)\n",
        "4. Compiling it into a runnable application\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Simple chatbot graph created!\n",
            "Flow: START ‚Üí chatbot ‚Üí END\n",
            "Ready to chat! üéâ\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Create the graph builder\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Step 2: Add our chatbot node\n",
        "graph_builder.add_node(\"chatbot\", chatbot_node)\n",
        "\n",
        "# Step 3: Define the flow\n",
        "# START -> chatbot -> END\n",
        "graph_builder.add_edge(START, \"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\", END)\n",
        "\n",
        "# Step 4: Compile the graph\n",
        "simple_chatbot = graph_builder.compile()\n",
        "\n",
        "print(\"‚úÖ Simple chatbot graph created!\")\n",
        "print(\"Flow: START ‚Üí chatbot ‚Üí END\")\n",
        "print(\"Ready to chat! üéâ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Test Your First LangGraph Agent!\n",
        "\n",
        "Let's test our simple chatbot. We'll send it a message and see how it responds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Testing Simple Chatbot\n",
            "========================================\n",
            "üë§ Human: Hello! My name is Niyantarana Tagore. What's your name?\n",
            "ü§ñ AI: Hello Niyantarana Tagore! Nice to meet you. My name is Assistant. How can I assist you today?\n",
            "\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "# Test our simple chatbot\n",
        "def test_simple_chatbot():\n",
        "    print(\"ü§ñ Testing Simple Chatbot\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Create initial state with a user message\n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=\"Hello! My name is Niyantarana Tagore. What's your name?\")]\n",
        "    }\n",
        "    \n",
        "    # Run the chatbot\n",
        "    result = simple_chatbot.invoke(initial_state)\n",
        "    \n",
        "    # Print the conversation\n",
        "    for i, message in enumerate(result[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ü§ñ AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    return result\n",
        "\n",
        "# Run the test\n",
        "first_result = test_simple_chatbot()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_simple_chatbot(user_input):\n",
        "    print(\"ü§ñ Testing Simple Chatbot\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    initial_state = {\n",
        "        \"messages\": [HumanMessage(content=user_input)]\n",
        "    }\n",
        "    \n",
        "    result = simple_chatbot.invoke(initial_state)\n",
        "    \n",
        "    for i, message in enumerate(result[\"messages\"]):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ü§ñ AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 40)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Testing Simple Chatbot\n",
            "========================================\n",
            "üë§ Human: What is the GATE cutoff for ECE?\n",
            "ü§ñ AI: The GATE cutoff for ECE (Electronics and Communication Engineering) varies every year depending on various factors such as the difficulty level of the exam, the number of applicants, and the number of available seats in different institutes. It is recommended to check the official GATE website or the website of the specific institute you are interested in for the most accurate and up-to-date information on GATE cutoff scores for ECE.\n",
            "\n",
            "========================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is the GATE cutoff for ECE?', additional_kwargs={}, response_metadata={}, id='bd6e74e9-f4c4-47f9-b3af-ec3804387e53'),\n",
              "  AIMessage(content='The GATE cutoff for ECE (Electronics and Communication Engineering) varies every year depending on various factors such as the difficulty level of the exam, the number of applicants, and the number of available seats in different institutes. It is recommended to check the official GATE website or the website of the specific institute you are interested in for the most accurate and up-to-date information on GATE cutoff scores for ECE.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 84, 'prompt_tokens': 17, 'total_tokens': 101, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bnhs9Z4fi1OHr2LlipSutNoG3eJEs', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--929f9750-021b-4caf-a743-659faad88416-0', usage_metadata={'input_tokens': 17, 'output_tokens': 84, 'total_tokens': 101, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_simple_chatbot(\"What is the GATE cutoff for ECE?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Testing Simple Chatbot\n",
            "========================================\n",
            "üë§ Human: What are the Subjects in GATE ECE?\n",
            "ü§ñ AI: The subjects in GATE ECE (Electronics and Communication Engineering) are as follows:\n",
            "\n",
            "1. Engineering Mathematics\n",
            "2. Networks, Signals, and Systems\n",
            "3. Electronic Devices\n",
            "4. Analog Circuits\n",
            "5. Digital Circuits\n",
            "6. Control Systems\n",
            "7. Communications\n",
            "8. Electromagnetics\n",
            "9. General Aptitude\n",
            "\n",
            "========================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What are the Subjects in GATE ECE?', additional_kwargs={}, response_metadata={}, id='4bde8332-c7c9-47d5-b1b0-db30d8d8ad2b'),\n",
              "  AIMessage(content='The subjects in GATE ECE (Electronics and Communication Engineering) are as follows:\\n\\n1. Engineering Mathematics\\n2. Networks, Signals, and Systems\\n3. Electronic Devices\\n4. Analog Circuits\\n5. Digital Circuits\\n6. Control Systems\\n7. Communications\\n8. Electromagnetics\\n9. General Aptitude', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 70, 'prompt_tokens': 17, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BnhsVkz4eyufRQCoLqYYUCVSNFTGp', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f3172def-1335-411b-a40e-112ca92309b7-0', usage_metadata={'input_tokens': 17, 'output_tokens': 70, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_simple_chatbot(\"What are the Subjects in GATE ECE?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üéâ Congratulations!\n",
        "\n",
        "You just built your first LangGraph agent! But there's a problem...\n",
        "\n",
        "**The Issue**: Our chatbot doesn't remember previous conversations. Each time we call it, it starts fresh.\n",
        "\n",
        "Let's test this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Memory Issue\n",
            "========================================\n",
            "üë§ Human: What was my name again?\n",
            "ü§ñ AI: I'm sorry, I don't have access to that information.\n",
            "\n",
            "‚ùå As you can see, the chatbot doesn't remember your name!\n",
            "This is because each call starts with a fresh state.\n",
            "In the next section, we'll fix this with **memory and checkpointing**! üß†\n"
          ]
        }
      ],
      "source": [
        "# Test memory issue - the chatbot won't remember the previous conversation\n",
        "print(\"üß™ Testing Memory Issue\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Second message - asking about the name mentioned earlier\n",
        "second_state = {\n",
        "    \"messages\": [HumanMessage(content=\"What was my name again?\")]\n",
        "}\n",
        "\n",
        "result2 = simple_chatbot.invoke(second_state)\n",
        "\n",
        "for message in result2[\"messages\"]:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"üë§ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"ü§ñ AI: {message.content}\")\n",
        "\n",
        "print(\"\\n‚ùå As you can see, the chatbot doesn't remember your name!\")\n",
        "print(\"This is because each call starts with a fresh state.\")\n",
        "print(\"In the next section, we'll fix this with **memory and checkpointing**! üß†\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß† Part 3: Adding Memory with Checkpointing {#part3}\n",
        "\n",
        "The power of LangGraph really shines when we add **persistent memory**. This allows our agent to:\n",
        "\n",
        "- Remember conversations across multiple interactions\n",
        "- Resume from where it left off\n",
        "- Handle long-running workflows\n",
        "- Support human-in-the-loop scenarios\n",
        "\n",
        "### What is Checkpointing?\n",
        "\n",
        "**Checkpointing** automatically saves the state after each step. When you invoke the graph again with the same `thread_id`, it loads the saved state and continues from there.\n",
        "\n",
        "Think of it like a video game save system! üéÆ\n",
        "\n",
        "### Step 1: Set Up a Checkpointer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langgraph-checkpoint-sqlite in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (2.0.10)\n",
            "Requirement already satisfied: aiosqlite>=0.20 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (0.21.0)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.0.21 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (2.0.26)\n",
            "Requirement already satisfied: sqlite-vec>=0.1.6 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint-sqlite) (0.1.6)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from aiosqlite>=0.20->langgraph-checkpoint-sqlite) (4.13.2)\n",
            "Requirement already satisfied: langchain-core>=0.2.38 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.3.66)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.10.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.11.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.23.0)\n",
            "Requirement already satisfied: anyio in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (4.9.0)\n",
            "Requirement already satisfied: certifi in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.0.9)\n",
            "Requirement already satisfied: idna in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from requests<3,>=2->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /Users/niyantarana/miniconda3/envs/bia_genai/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core>=0.2.38->langgraph-checkpoint>=2.0.21->langgraph-checkpoint-sqlite) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph-checkpoint-sqlite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Checkpointer created!\n",
            "This will automatically save and restore conversation state\n",
            "In production, you'd use a real database file instead of ':memory:'\n"
          ]
        }
      ],
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()\n",
        "\n",
        "\n",
        "print(\"‚úÖ Checkpointer created!\")\n",
        "print(\"This will automatically save and restore conversation state\")\n",
        "print(\"In production, you'd use a real database file instead of ':memory:'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create a Chatbot with Memory\n",
        "\n",
        "Now let's rebuild our chatbot with memory capabilities:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "graph_builder_with_memory = StateGraph(State)\n",
        "\n",
        "graph_builder_with_memory.add_node(\"chatbot\", chatbot_node)\n",
        "graph_builder_with_memory.add_edge(START, \"chatbot\")\n",
        "graph_builder_with_memory.add_edge(\"chatbot\", END)\n",
        "\n",
        "chatbot_with_memory = graph_builder_with_memory.compile(checkpointer=memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique ID for this conversation: f53ef741-dd09-41b3-96b9-6b5997197d5f\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "unique_id = uuid.uuid4()\n",
        "str(unique_id)\n",
        "print(f\"Unique ID for this conversation: {unique_id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "unique_id = \"f53ef741-dd09-41b3-96b9-6b5997197d5f\"\n",
        "def test_simple(user_input):\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\":str(unique_id)}}\n",
        "    result = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=user_input)]},\n",
        "        config\n",
        "    )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Test the Memory\n",
        "\n",
        "The key to using memory is the **config** parameter with a `thread_id`. All conversations with the same `thread_id` will share memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† Testing Chatbot with Memory\n",
            "==================================================\n",
            "üìù First interaction:\n",
            "üë§ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "üë§ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "üë§ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "ü§ñ AI: Hello, Niyantarana Tagore! It's great to meet another AI Agent Developer. What kind of projects are you currently working on in the AI field?\n",
            "\n",
            "üìù Second interaction (same thread_id):\n",
            "üë§ Human: What's my name and what I do?\n",
            "ü§ñ AI: Your name is Niyantarana Tagore, and you are an AI Agent Developer.\n",
            "\n",
            "‚úÖ Success! The chatbot remembered both your name and Specialization!\n"
          ]
        }
      ],
      "source": [
        "def test_memory_chatbot():\n",
        "    print(\"üß† Testing Chatbot with Memory\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Configuration with thread_id - this is crucial for memory!\n",
        "    config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "    \n",
        "    # First message\n",
        "    print(\"üìù First interaction:\")\n",
        "    result1 = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\")]},\n",
        "        config  # Pass config as second parameter!\n",
        "    )\n",
        "    \n",
        "    for message in result1[\"messages\"]:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ü§ñ AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\nüìù Second interaction (same thread_id):\")\n",
        "    result2 = chatbot_with_memory.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"What's my name and what I do?\")]},\n",
        "        config  # Same config = same memory!\n",
        "    )\n",
        "    \n",
        "    # Only show the new messages\n",
        "    new_messages = result2[\"messages\"][len(result1[\"messages\"]):]\n",
        "    for message in new_messages:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ü§ñ AI: {message.content}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Success! The chatbot remembered both your name and Specialization!\")\n",
        "    return result2\n",
        "\n",
        "# Test the memory\n",
        "memory_result = test_memory_chatbot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üîç Understanding Thread IDs\n",
        "\n",
        "Different `thread_id`s create separate conversations. Let's test this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Testing Different Thread ID\n",
            "========================================\n",
            "üë§ Human: What's my name?\n",
            "ü§ñ AI: I'm sorry, I do not know your name as I am an AI assistant and do not have access to personal information.\n",
            "\n",
            "üéØ Key Insight:\n",
            "- Same thread_id = shared memory\n",
            "- Different thread_id = separate conversations\n",
            "- This allows multiple users or conversation contexts!\n"
          ]
        }
      ],
      "source": [
        "# Test with a different thread_id\n",
        "print(\"üîÑ Testing Different Thread ID\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Different thread_id = fresh conversation\n",
        "different_config = {\"configurable\": {\"thread_id\": \"conversation_2\"}}\n",
        "\n",
        "result_different = chatbot_with_memory.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What's my name?\")]},\n",
        "    different_config  # Different thread_id!\n",
        ")\n",
        "\n",
        "for message in result_different[\"messages\"]:\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"üë§ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"ü§ñ AI: {message.content}\")\n",
        "\n",
        "print(\"\\nüéØ Key Insight:\")\n",
        "print(\"- Same thread_id = shared memory\")\n",
        "print(\"- Different thread_id = separate conversations\")\n",
        "print(\"- This allows multiple users or conversation contexts!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìä Inspecting the State\n",
        "\n",
        "You can inspect the current state of any conversation using `get_state()`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä State Inspection\n",
            "==============================\n",
            "Number of messages: 6\n",
            "Next node to execute: ()\n",
            "Thread ID: conversation_1\n",
            "\n",
            "üí¨ Full conversation history:\n",
            "1. üë§ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "2. üë§ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "3. üë§ Human: Hi! My name is Niyantarana Tagore and I am an AI Agent Developer.\n",
            "4. ü§ñ AI: Hello, Niyantarana Tagore! It's great to meet another AI Agent Developer. What kind of projects are ...\n",
            "5. üë§ Human: What's my name and what I do?\n",
            "6. ü§ñ AI: Your name is Niyantarana Tagore, and you are an AI Agent Developer....\n"
          ]
        }
      ],
      "source": [
        "# Inspect the state of our first conversation\n",
        "config1 = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
        "state_snapshot = chatbot_with_memory.get_state(config1)\n",
        "\n",
        "print(\"üìä State Inspection\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"Number of messages: {len(state_snapshot.values['messages'])}\")\n",
        "print(f\"Next node to execute: {state_snapshot.next}\")\n",
        "print(f\"Thread ID: {state_snapshot.config['configurable']['thread_id']}\")\n",
        "\n",
        "print(\"\\nüí¨ Full conversation history:\")\n",
        "for i, message in enumerate(state_snapshot.values[\"messages\"], 1):\n",
        "    if isinstance(message, HumanMessage):\n",
        "        print(f\"{i}. üë§ Human: {message.content}\")\n",
        "    elif isinstance(message, AIMessage):\n",
        "        print(f\"{i}. ü§ñ AI: {message.content[:100]}...\")  # Truncate for readability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üõ†Ô∏è Part 4: Tool Integration & External APIs {#part4}\n",
        "\n",
        "Real AI agents need to interact with the outside world! Let's add tool calling capabilities to our chatbot.\n",
        "\n",
        "### What are Tools?\n",
        "\n",
        "Tools are functions that your AI agent can call to:\n",
        "- Search the web\n",
        "- Query databases\n",
        "- Send emails\n",
        "- Perform calculations\n",
        "- Access APIs\n",
        "- And much more!\n",
        "\n",
        "### Step 1: Create a Simple Tool\n",
        "\n",
        "Let's start with a simple calculator tool:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üßÆ Testing Calculator Tool:\n",
            "The result of '2 + 3 * 4' is 14\n",
            "The result of 'sqrt(16) + 5' is 9.0\n",
            "\n",
            "‚è∞ Testing Time Tool:\n",
            "Current time is: 2025-06-29 15:23:54\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.tools import tool\n",
        "from datetime import datetime\n",
        "import math\n",
        "\n",
        "@tool\n",
        "def calculator(expression: str) -> str:\n",
        "    \"\"\"\n",
        "    Safely evaluate a mathematical expression using allowed math functions.\n",
        "    \n",
        "    Args:\n",
        "        expression (str): A mathematical expression to evaluate (e.g., \"2 + 3 * 4\")\n",
        "        \n",
        "    Returns:\n",
        "        str: The result or an error message\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Define safe functions from math module\n",
        "        safe_math = {name: obj for name, obj in math.__dict__.items() if not name.startswith(\"__\")}\n",
        "        # Include built-in functions that are safe\n",
        "        safe_math.update({\"abs\": abs, \"round\": round})\n",
        "\n",
        "        # Evaluate expression in restricted environment\n",
        "        result = eval(expression, {\"__builtins__\": {}}, safe_math)\n",
        "        return f\"The result of '{expression}' is {result}\"\n",
        "    except Exception as e:\n",
        "        return f\"Error evaluating '{expression}': {e}\"\n",
        "\n",
        "@tool  \n",
        "def get_current_time() -> str:\n",
        "    \"\"\"\n",
        "    Return the current system time in YYYY-MM-DD HH:MM:SS format.\n",
        "    \n",
        "    Returns:\n",
        "        str: The current time\n",
        "    \"\"\"\n",
        "    return f\"Current time is: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
        "\n",
        "# Demo usage\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üßÆ Testing Calculator Tool:\")\n",
        "    print(calculator.invoke({\"expression\": \"2 + 3 * 4\"}))\n",
        "    print(calculator.invoke({\"expression\": \"sqrt(16) + 5\"}))\n",
        "\n",
        "    print(\"\\n‚è∞ Testing Time Tool:\")\n",
        "    print(get_current_time.invoke({}))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create a Tool-Enabled Chatbot\n",
        "\n",
        "Now we need to:\n",
        "1. Bind tools to our LLM\n",
        "2. Create a tool-calling node\n",
        "3. Add conditional logic to decide when to use tools\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tool-enabled chatbot components created!\n",
            "üîß Tools bound to LLM\n",
            "üß† Chatbot node can interpret and generate tool usage\n",
            "‚öôÔ∏è Tool node can execute tool invocations correctly\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, Any\n",
        "from langgraph.prebuilt import ToolNode\n",
        "from langchain_core.messages import ToolMessage\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "# Assumed: `llm`, `calculator`, and `get_current_time` are already defined/imported\n",
        "\n",
        "# Step 1: Define tools and bind to LLM\n",
        "tools = [calculator, get_current_time]\n",
        "llm_with_tools: Runnable = llm.bind_tools(tools)\n",
        "\n",
        "# Step 2: Define chatbot node that processes messages\n",
        "def chatbot_with_tools(state: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    A chatbot node that uses LLM with tool awareness to generate a response.\n",
        "    \n",
        "    Args:\n",
        "        state (dict): A dictionary containing 'messages' (list of messages)\n",
        "    \n",
        "    Returns:\n",
        "        dict: Updated state with new response message\n",
        "    \"\"\"\n",
        "    messages = state.get(\"messages\", [])\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Step 3: Create LangGraph ToolNode to execute tool calls\n",
        "tool_node = ToolNode(tools)\n",
        "\n",
        "# Confirmation prints\n",
        "print(\"‚úÖ Tool-enabled chatbot components created!\")\n",
        "print(\"üîß Tools bound to LLM\")\n",
        "print(\"üß† Chatbot node can interpret and generate tool usage\")\n",
        "print(\"‚öôÔ∏è Tool node can execute tool invocations correctly\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Add Conditional Logic\n",
        "\n",
        "We need to route between:\n",
        "- **Tools**: If the LLM wants to use a tool\n",
        "- **End**: If the LLM gives a final response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Conditional logic created!\n",
            "This function decides whether to:\n",
            "- üîß Use tools (if LLM made tool calls)\n",
            "- üõë End conversation (if LLM gave final response)\n"
          ]
        }
      ],
      "source": [
        "from typing import Dict, Any, List, Literal, Union\n",
        "\n",
        "# Assuming a message structure compatible with LangChain's AIMessage/ChatMessage types\n",
        "def should_continue(state: Dict[str, Any]) -> Literal[\"tools\", \"__end__\"]:\n",
        "    \"\"\"\n",
        "    Decide whether to proceed with tool execution or end the conversation.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current conversation state containing 'messages'.\n",
        "\n",
        "    Returns:\n",
        "        Literal[\"tools\", \"__end__\"]: \n",
        "            - \"tools\" if the last message includes tool calls.\n",
        "            - \"__end__\" if there are no tool calls (i.e., conversation is finished).\n",
        "    \"\"\"\n",
        "    messages: List[Any] = state.get(\"messages\", [])\n",
        "    \n",
        "    if not messages:\n",
        "        return \"__end__\"\n",
        "    \n",
        "    last_message = messages[-1]\n",
        "\n",
        "    # Check if the last message includes tool calls\n",
        "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
        "        return \"tools\"\n",
        "    \n",
        "    return \"__end__\"\n",
        "\n",
        "# Confirmation output\n",
        "print(\"‚úÖ Conditional logic created!\")\n",
        "print(\"This function decides whether to:\")\n",
        "print(\"- üîß Use tools (if LLM made tool calls)\")\n",
        "print(\"- üõë End conversation (if LLM gave final response)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Build the Tool-Enabled Graph\n",
        "\n",
        "Now let's put it all together:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Tool-enabled chatbot with memory created!\n",
            "üìà Flow:\n",
            "   START ‚Üí chatbot ‚Üí [tools | __end__]\n",
            "     ‚Ü≥ if tools ‚Üí tools ‚Üí chatbot ‚Üí ...\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import StateGraph, START, END\n",
        "\n",
        "# Step 1: Initialize the graph builder with state type\n",
        "tool_graph_builder = StateGraph(State)\n",
        "\n",
        "# Step 2: Add functional and tool nodes\n",
        "tool_graph_builder.add_node(\"chatbot\", chatbot_with_tools)\n",
        "tool_graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "# Step 3: Define edges\n",
        "tool_graph_builder.add_edge(START, \"chatbot\")  # Start ‚Üí chatbot\n",
        "\n",
        "# Step 4: Conditional routing after chatbot response\n",
        "tool_graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",               # From this node\n",
        "    should_continue,         # Conditional function\n",
        "    {\n",
        "        \"tools\": \"tools\",    # If tool call present ‚Üí tools\n",
        "        \"__end__\": END       # Else ‚Üí END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Step 5: Always return from tools ‚Üí chatbot\n",
        "tool_graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "# Step 6: Compile with memory (checkpointer)\n",
        "tool_chatbot = tool_graph_builder.compile(checkpointer=memory)\n",
        "\n",
        "# Confirmation\n",
        "print(\"‚úÖ Tool-enabled chatbot with memory created!\")\n",
        "print(\"üìà Flow:\")\n",
        "print(\"   START ‚Üí chatbot ‚Üí [tools | __end__]\")\n",
        "print(\"     ‚Ü≥ if tools ‚Üí tools ‚Üí chatbot ‚Üí ...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è Testing Tool-Enabled Chatbot\n",
            "==================================================\n",
            "üìù User Input: What is the Agentic AI Summit about?\n",
            "\n",
            "üì® Chat History:\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool `calculator`\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool `calculator`\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool `get_current_time`\n",
            "üîß Tool Response: Current time is: 2025-06-29 14:28:07\n",
            "ü§ñ AI: The current time is 14:28:07 on June 29, 2025.\n",
            "üë§ Human: who is PM of INdia\n",
            "ü§ñ AI: I'm an AI assistant and I can provide information based on the latest available data. As of my last update, the Prime Minister of India is Narendra Modi.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool `calculator`\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool `calculator`\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool `get_current_time`\n",
            "üîß Tool Response: Current time is: 2025-06-29 15:04:12\n",
            "ü§ñ AI: The current time is 15:04:12 on June 29, 2025.\n",
            "üë§ Human: Where was Agentic AI Summit 2025 held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit 2025. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: Where was Agentic AI Summit held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool `calculator`\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool `calculator`\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool `get_current_time`\n",
            "üîß Tool Response: Current time is: 2025-06-29 15:07:12\n",
            "ü§ñ AI: The current time is 15:07:12 on June 29, 2025.\n",
            "üë§ Human: Where was Agentic AI Summit held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: What is the Agentic AI Summit about?\n",
            "ü§ñ AI: The Agentic AI Summit is an event focused on artificial intelligence (AI) and its applications across various industries. It likely covers topics such as AI advancements, machine learning, data analytics, automation, and the impact of AI on businesses and society. For specific details about the summit's agenda and themes, you may need to refer to the event's official website or promotional materials.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "from typing import Union\n",
        "\n",
        "def test_tool_chatbot(user_message: str) -> None:\n",
        "    \"\"\"\n",
        "    Test the tool-enabled chatbot with a single user message.\n",
        "    \n",
        "    Args:\n",
        "        user_message (str): The input message from the user.\n",
        "    \"\"\"\n",
        "    print(\"üõ†Ô∏è Testing Tool-Enabled Chatbot\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"tool_test_1\"}}\n",
        "    messages = [HumanMessage(content=user_message)]\n",
        "    \n",
        "    print(f\"üìù User Input: {user_message}\")\n",
        "    \n",
        "    try:\n",
        "        result = tool_chatbot.invoke({\"messages\": messages}, config=config)\n",
        "        print(\"\\nüì® Chat History:\")\n",
        "        \n",
        "        for msg in result.get(\"messages\", []):\n",
        "            if isinstance(msg, HumanMessage):\n",
        "                print(f\"üë§ Human: {msg.content}\")\n",
        "            elif isinstance(msg, AIMessage):\n",
        "                if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n",
        "                    tool_name = msg.tool_calls[0][\"name\"]\n",
        "                    print(f\"ü§ñ AI (Tool Call): Calling tool `{tool_name}`\")\n",
        "                else:\n",
        "                    print(f\"ü§ñ AI: {msg.content}\")\n",
        "            elif isinstance(msg, ToolMessage):\n",
        "                print(f\"üîß Tool Response: {msg.content}\")\n",
        "            else:\n",
        "                print(f\"üì¶ Unknown Message Type: {msg}\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error during chatbot invocation: {e}\")\n",
        "\n",
        "# Run the test\n",
        "tool_result = test_tool_chatbot(\"What is the Agentic AI Summit about?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Test the Tool-Enabled Chatbot\n",
        "\n",
        "Let's test our enhanced chatbot with some math problems and time queries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üõ†Ô∏è Testing Tool-Enabled Chatbot\n",
            "==================================================\n",
            "üìù Test 1: Math Calculation\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool 'get_current_time'\n",
            "üîß Tool Response: Current time is: 2025-06-29 14:28:07\n",
            "ü§ñ AI: The current time is 14:28:07 on June 29, 2025.\n",
            "üë§ Human: who is PM of INdia\n",
            "ü§ñ AI: I'm an AI assistant and I can provide information based on the latest available data. As of my last update, the Prime Minister of India is Narendra Modi.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool 'get_current_time'\n",
            "üîß Tool Response: Current time is: 2025-06-29 15:04:12\n",
            "ü§ñ AI: The current time is 15:04:12 on June 29, 2025.\n",
            "üë§ Human: Where was Agentic AI Summit 2025 held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit 2025. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: Where was Agentic AI Summit held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool 'get_current_time'\n",
            "üîß Tool Response: Current time is: 2025-06-29 15:07:12\n",
            "ü§ñ AI: The current time is 15:07:12 on June 29, 2025.\n",
            "üë§ Human: Where was Agentic AI Summit held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: What is the Agentic AI Summit about?\n",
            "ü§ñ AI: The Agentic AI Summit is an event focused on artificial intelligence (AI) and its applications across various industries. It likely covers topics such as AI advancements, machine learning, data analytics, automation, and the impact of AI on businesses and society. For specific details about the summit's agenda and themes, you may need to refer to the event's official website or promotional materials.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of '15 * 7' is 105\n",
            "üîß Tool Response: The result of 'sqrt(144)' is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of '105 + 12' is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "\n",
            "üìù Test 2: Current Time\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool 'get_current_time'\n",
            "üîß Tool Response: Current time is: 2025-06-29 14:28:07\n",
            "ü§ñ AI: The current time is 14:28:07 on June 29, 2025.\n",
            "üë§ Human: who is PM of INdia\n",
            "ü§ñ AI: I'm an AI assistant and I can provide information based on the latest available data. As of my last update, the Prime Minister of India is Narendra Modi.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool 'get_current_time'\n",
            "üîß Tool Response: Current time is: 2025-06-29 15:04:12\n",
            "ü§ñ AI: The current time is 15:04:12 on June 29, 2025.\n",
            "üë§ Human: Where was Agentic AI Summit 2025 held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit 2025. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: Where was Agentic AI Summit held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 15 * 7 is 105\n",
            "üîß Tool Response: The result of sqrt(144) is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of 105 + 12 is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool 'get_current_time'\n",
            "üîß Tool Response: Current time is: 2025-06-29 15:07:12\n",
            "ü§ñ AI: The current time is 15:07:12 on June 29, 2025.\n",
            "üë§ Human: Where was Agentic AI Summit held?\n",
            "ü§ñ AI: I do not have information about the specific location of the Agentic AI Summit. You may need to check the official website or event details for the location information.\n",
            "üë§ Human: What is the Agentic AI Summit about?\n",
            "ü§ñ AI: The Agentic AI Summit is an event focused on artificial intelligence (AI) and its applications across various industries. It likely covers topics such as AI advancements, machine learning, data analytics, automation, and the impact of AI on businesses and society. For specific details about the summit's agenda and themes, you may need to refer to the event's official website or promotional materials.\n",
            "üë§ Human: What's 15 * 7 + sqrt(144)?\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of '15 * 7' is 105\n",
            "üîß Tool Response: The result of 'sqrt(144)' is 12.0\n",
            "ü§ñ AI (Tool Call): Calling tool 'calculator'\n",
            "üîß Tool Response: The result of '105 + 12' is 117\n",
            "ü§ñ AI: The result of \\(15 \\times 7 + \\sqrt{144}\\) is \\(117\\).\n",
            "üë§ Human: What time is it right now?\n",
            "ü§ñ AI (Tool Call): Calling tool 'get_current_time'\n",
            "üîß Tool Response: Current time is: 2025-06-29 15:28:52\n",
            "ü§ñ AI: The current time is 15:28:52 on June 29, 2025.\n",
            "\n",
            "‚úÖ Tool integration working perfectly!\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "def test_tool_chatbot():\n",
        "    print(\"üõ†Ô∏è Testing Tool-Enabled Chatbot\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"tool_test_1\"}}\n",
        "    \n",
        "    # Test 1: Math calculation\n",
        "    print(\"üìù Test 1: Math Calculation\")\n",
        "    messages_1 = [HumanMessage(content=\"What's 15 * 7 + sqrt(144)?\")]\n",
        "    result1 = tool_chatbot.invoke({\"messages\": messages_1}, config)\n",
        "\n",
        "    for message in result1.get(\"messages\", []):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            if getattr(message, \"tool_calls\", None):\n",
        "                for tool_call in message.tool_calls:\n",
        "                    print(f\"ü§ñ AI (Tool Call): Calling tool '{tool_call['name']}'\")\n",
        "            else:\n",
        "                print(f\"ü§ñ AI: {message.content}\")\n",
        "        elif isinstance(message, ToolMessage):\n",
        "            print(f\"üîß Tool Response: {message.content}\")\n",
        "    \n",
        "    # Separator\n",
        "    print(\"\\nüìù Test 2: Current Time\")\n",
        "    messages_2 = [HumanMessage(content=\"What time is it right now?\")]\n",
        "    result2 = tool_chatbot.invoke({\"messages\": messages_2}, config)\n",
        "\n",
        "    for message in result2.get(\"messages\", []):\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            if getattr(message, \"tool_calls\", None):\n",
        "                for tool_call in message.tool_calls:\n",
        "                    print(f\"ü§ñ AI (Tool Call): Calling tool '{tool_call['name']}'\")\n",
        "            else:\n",
        "                print(f\"ü§ñ AI: {message.content}\")\n",
        "        elif isinstance(message, ToolMessage):\n",
        "            print(f\"üîß Tool Response: {message.content}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ Tool integration working perfectly!\")\n",
        "    return result2\n",
        "\n",
        "# Run the test\n",
        "tool_result = test_tool_chatbot()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ù Part 5: Human-in-the-Loop Workflows {#part5}\n",
        "\n",
        "Sometimes AI agents need human oversight or approval before taking actions. LangGraph makes this easy with **interrupts**.\n",
        "\n",
        "### What are Interrupts?\n",
        "\n",
        "Interrupts pause the graph execution at specific nodes, allowing humans to:\n",
        "- Review what the agent plans to do\n",
        "- Modify the state if needed\n",
        "- Approve or reject actions\n",
        "- Provide additional guidance\n",
        "\n",
        "### Step 1: Create an Agent that Asks for Help\n",
        "\n",
        "Let's create an agent that can request human assistance when it's unsure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Human-in-the-loop tools created!\n",
            "üîÅ The agent can now escalate issues to a human when needed.\n"
          ]
        }
      ],
      "source": [
        "# üì¶ Define enhanced state that tracks both messages and human loop flag\n",
        "class HumanLoopState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    ask_human: bool  # Whether human help was requested\n",
        "\n",
        "# üõ†Ô∏è Tool: Request human assistance\n",
        "@tool\n",
        "def request_human_help(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Request help from a human supervisor.\n",
        "\n",
        "    Args:\n",
        "        question: The question or situation where human help is needed\n",
        "\n",
        "    Returns:\n",
        "        A confirmation message that help has been requested.\n",
        "    \"\"\"\n",
        "    return f\"‚úÖ Human help requested for: \\\"{question}\\\"\"\n",
        "\n",
        "# üìö Combine all tools including human assistance\n",
        "human_tools = [calculator, get_current_time, request_human_help]\n",
        "\n",
        "# üß† Bind tools to LLM\n",
        "llm_with_human_tools = llm.bind_tools(human_tools)\n",
        "\n",
        "print(\"‚úÖ Human-in-the-loop tools created!\")\n",
        "print(\"üîÅ The agent can now escalate issues to a human when needed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Create Nodes with Human Assistance Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Human-in-the-loop nodes created!\n",
            "- chatbot_with_human_help: Can request human assistance\n",
            "- human_node: Handles human intervention\n"
          ]
        }
      ],
      "source": [
        "def chatbot_with_human_help(state: HumanLoopState) -> Dict[str, Any]:\n",
        "    \"\"\"Chatbot that can request human help.\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm_with_human_tools.invoke(messages)\n",
        "\n",
        "    # Check if the agent requested human help\n",
        "    ask_human = any(\n",
        "        tool_call.get('name') == 'request_human_help' \n",
        "        for tool_call in getattr(response, 'tool_calls', [])\n",
        "    )\n",
        "\n",
        "    return {\"messages\": [response], \"ask_human\": ask_human}\n",
        "\n",
        "def human_node(state: HumanLoopState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Human intervention node - this is where humans can provide input.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        tool_call = last_message.tool_calls[0]\n",
        "        if tool_call['name'] == 'request_human_help':\n",
        "            # Simulate human response\n",
        "            human_response = (\n",
        "                \"I've reviewed your question. Please proceed with the calculation \"\n",
        "                \"and provide a detailed explanation.\"\n",
        "            )\n",
        "            \n",
        "            # Create a tool message with the human's response\n",
        "            tool_message = ToolMessage(\n",
        "                content=human_response,\n",
        "                tool_call_id=tool_call['id']\n",
        "            )\n",
        "            return {\"messages\": [tool_message], \"ask_human\": False}\n",
        "\n",
        "    return {\"ask_human\": False}\n",
        "\n",
        "print(\"‚úÖ Human-in-the-loop nodes created!\")\n",
        "print(\"- chatbot_with_human_help: Can request human assistance\")\n",
        "print(\"- human_node: Handles human intervention\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Human-in-the-loop nodes created!\n",
            "- chatbot_with_human_help: Can request human assistance\n",
            "- human_node: Handles human intervention\n"
          ]
        }
      ],
      "source": [
        "def chatbot_with_human_help(state: HumanLoopState) -> Dict[str, Any]:\n",
        "    \"\"\"Chatbot that can request human help\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm_with_human_tools.invoke(messages)\n",
        "    \n",
        "    # Check if the agent requested human help\n",
        "    ask_human = False\n",
        "    if hasattr(response, 'tool_calls') and response.tool_calls:\n",
        "        for tool_call in response.tool_calls:\n",
        "            if tool_call['name'] == 'request_human_help':\n",
        "                ask_human = True\n",
        "                break\n",
        "    \n",
        "    return {\"messages\": [response], \"ask_human\": ask_human}\n",
        "\n",
        "def human_node(state: HumanLoopState) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Human intervention node - this is where humans can provide input\n",
        "    \"\"\"\n",
        "    # In a real application, this would wait for human input\n",
        "    # For this demo, we'll simulate human response\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        tool_call = last_message.tool_calls[0]\n",
        "        if tool_call['name'] == 'request_human_help':\n",
        "            # Simulate human response\n",
        "            human_response = \"I've reviewed your question. Please proceed with the calculation and provide a detailed explanation.\"\n",
        "            \n",
        "            # Create a tool message with the human's response\n",
        "            tool_message = ToolMessage(\n",
        "                content=human_response,\n",
        "                tool_call_id=tool_call['id']\n",
        "            )\n",
        "            return {\"messages\": [tool_message], \"ask_human\": False}\n",
        "    \n",
        "    return {\"ask_human\": False}\n",
        "\n",
        "print(\"‚úÖ Human-in-the-loop nodes created!\")\n",
        "print(\"- chatbot_with_human_help: Can request human assistance\")\n",
        "print(\"- human_node: Handles human intervention\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Build the Human-in-the-Loop Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Human-in-the-loop chatbot created!\n",
            "Key feature: interrupt_before=['human'] pauses for human input\n"
          ]
        }
      ],
      "source": [
        "def route_human_loop(state: HumanLoopState) -> Literal[\"human\", \"tools\", \"__end__\"]:\n",
        "    \"\"\"\n",
        "    Decide the next node: human intervention, tool use, or end.\n",
        "    \"\"\"\n",
        "    \n",
        "    if state.get(\"ask_human\", False):\n",
        "        return \"human\"\n",
        "    \n",
        "    messages = state[\"messages\"]\n",
        "    last_message = messages[-1]\n",
        "    \n",
        "    # Check for tool calls (excluding request_human_help)\n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            if tool_call['name'] != 'request_human_help':\n",
        "                return \"tools\"\n",
        "    \n",
        "    return \"__end__\"\n",
        "\n",
        "# Build the graph\n",
        "human_loop_builder = StateGraph(HumanLoopState)\n",
        "\n",
        "# Add nodes\n",
        "human_loop_builder.add_node(\"chatbot\", chatbot_with_human_help)\n",
        "human_loop_builder.add_node(\"tools\", ToolNode(human_tools))\n",
        "human_loop_builder.add_node(\"human\", human_node)\n",
        "\n",
        "# Add edges\n",
        "human_loop_builder.add_edge(START, \"chatbot\")\n",
        "\n",
        "# Add conditional routing\n",
        "human_loop_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    route_human_loop,\n",
        "    {\n",
        "        \"human\": \"human\",\n",
        "        \"tools\": \"tools\", \n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# After human or tools, go back to chatbot\n",
        "human_loop_builder.add_edge(\"human\", END)\n",
        "human_loop_builder.add_edge(\"tools\", END)\n",
        "\n",
        "# Compile with interrupt before human node\n",
        "human_loop_chatbot = human_loop_builder.compile(\n",
        "    checkpointer=memory,\n",
        "    interrupt_before=[\"human\"]  # This pauses execution before human node\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Human-in-the-loop chatbot created!\")\n",
        "print(\"Key feature: interrupt_before=['human'] pauses for human input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(human_loop_chatbot.get_graph().draw_mermaid_png()))\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Test Human-in-the-Loop\n",
        "\n",
        "Let's test the interrupt functionality:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ù Testing Human-in-the-Loop\n",
            "========================================\n",
            "üìù Asking agent to request human help:\n",
            "üë§ Human: I need help with a complex decision. Can you request human assistance?\n",
            "ü§ñ AI: [Requesting human help]\n",
            "\n",
            "üîç Current state:\n",
            "Next node to execute: ('human',)\n",
            "Ask human flag: True\n",
            "\n",
            "‚úÖ Execution interrupted! Human intervention required.\n",
            "In a real app, a human would now review and provide input.\n",
            "\n",
            "‚ñ∂Ô∏è Continuing execution (simulating human input)...\n",
            "üë®‚Äçüíº Human: üë®‚Äçüè´ Human: I've reviewed your request. Please proceed with the computation and provide a step-by-step explanation.\n",
            "ü§ñ AI: I have requested human assistance for your complex decision. Now, I will assist you with the computation. What specific calculations or decisions do you need help with?\n"
          ]
        }
      ],
      "source": [
        "def test_human_loop():\n",
        "    print(\"ü§ù Testing Human-in-the-Loop\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"human_loop_test\"}}\n",
        "    \n",
        "    # Request that requires human help\n",
        "    print(\"üìù Asking agent to request human help:\")\n",
        "    \n",
        "    initial_result = human_loop_chatbot.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"I need help with a complex decision. Can you request human assistance?\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    for message in initial_result[\"messages\"]:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            if hasattr(message, 'tool_calls') and message.tool_calls:\n",
        "                print(f\"ü§ñ AI: [Requesting human help]\")\n",
        "            else:\n",
        "                print(f\"ü§ñ AI: {message.content}\")\n",
        "    \n",
        "    # Check if execution was interrupted\n",
        "    state = human_loop_chatbot.get_state(config)\n",
        "    print(f\"\\nüîç Current state:\")\n",
        "    print(f\"Next node to execute: {state.next}\")\n",
        "    print(f\"Ask human flag: {state.values.get('ask_human', False)}\")\n",
        "    \n",
        "    if state.next == (\"human\",):\n",
        "        print(\"\\n‚úÖ Execution interrupted! Human intervention required.\")\n",
        "        print(\"In a real app, a human would now review and provide input.\")\n",
        "        \n",
        "        # Continue execution (simulating human approval)\n",
        "        print(\"\\n‚ñ∂Ô∏è Continuing execution (simulating human input)...\")\n",
        "        final_result = human_loop_chatbot.invoke(None, config)\n",
        "        \n",
        "        # Show the final messages\n",
        "        new_messages = final_result[\"messages\"][len(initial_result[\"messages\"]):]\n",
        "        for message in new_messages:\n",
        "            if isinstance(message, ToolMessage):\n",
        "                print(f\"üë®‚Äçüíº Human: {message.content}\")\n",
        "            elif isinstance(message, AIMessage):\n",
        "                print(f\"ü§ñ AI: {message.content}\")\n",
        "    \n",
        "    return final_result\n",
        "\n",
        "# Test the human-in-the-loop functionality\n",
        "human_loop_result = test_human_loop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Part 6: Advanced State Management {#part6}\n",
        "\n",
        "So far, we've used simple state with just messages. But LangGraph can handle much more complex state for sophisticated applications.\n",
        "\n",
        "### Custom State Example: Research Assistant\n",
        "\n",
        "Let's build a research assistant that:\n",
        "- Tracks research topics\n",
        "- Maintains a list of findings\n",
        "- Counts the number of searches performed\n",
        "- Has a confidence score for its conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Advanced research assistant tools created!\n",
            "Tools available: web_search, analyze_findings, calculator, get_current_time\n"
          ]
        }
      ],
      "source": [
        "# Advanced State for Research Assistant\n",
        "class ResearchState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    research_topic: str\n",
        "    findings: List[str]\n",
        "    search_count: int\n",
        "    confidence_score: float\n",
        "    max_searches: int\n",
        "\n",
        "# Research tools\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Simulate a web search (in real app, use actual search API).\n",
        "    \n",
        "    Args:\n",
        "        query: Search query\n",
        "        \n",
        "    Returns:\n",
        "        Search results\n",
        "    \"\"\"\n",
        "    # Simulate search results\n",
        "    results = {\n",
        "        \"python\": \"Python is a high-level programming language known for its simplicity and versatility.\",\n",
        "        \"machine learning\": \"Machine learning is a subset of AI that enables computers to learn from data.\",\n",
        "        \"langgraph\": \"LangGraph is a library for building stateful, multi-actor applications with LLMs.\",\n",
        "        \"default\": f\"Search results for '{query}': This is simulated search data with relevant information.\"\n",
        "    }\n",
        "    \n",
        "    return results.get(query.lower(), results[\"default\"])\n",
        "\n",
        "@tool\n",
        "def analyze_findings(findings_list: str) -> str:\n",
        "    \"\"\"\n",
        "    Analyze research findings and provide insights.\n",
        "    \n",
        "    Args:\n",
        "        findings_list: Comma-separated list of findings\n",
        "        \n",
        "    Returns:\n",
        "        Analysis of the findings\n",
        "    \"\"\"\n",
        "    findings = findings_list.split(',')\n",
        "    analysis = f\"Analysis of {len(findings)} findings: \"\n",
        "    \n",
        "    if len(findings) >= 3:\n",
        "        analysis += \"Comprehensive research completed. Strong evidence base.\"\n",
        "    elif len(findings) >= 2:\n",
        "        analysis += \"Good research foundation. Consider additional sources.\"\n",
        "    else:\n",
        "        analysis += \"Limited research. More investigation needed.\"\n",
        "    \n",
        "    return analysis\n",
        "\n",
        "# Research tools list\n",
        "research_tools = [web_search, analyze_findings, calculator, get_current_time]\n",
        "research_llm = llm.bind_tools(research_tools)\n",
        "\n",
        "print(\"‚úÖ Advanced research assistant tools created!\")\n",
        "print(\"Tools available: web_search, analyze_findings, calculator, get_current_time\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Research Assistant Nodes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Research assistant nodes created!\n",
            "- research_chatbot: Handles conversations with research context\n",
            "- research_tool_node: Executes tools and updates research state\n",
            "- initialize_research: Sets up initial research parameters\n"
          ]
        }
      ],
      "source": [
        "def research_chatbot(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Research chatbot with advanced state management\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    \n",
        "    # Add context about current research state\n",
        "    context_message = f\"\"\"\n",
        "Current research status:\n",
        "- Topic: {state.get('research_topic', 'Not set')}\n",
        "- Findings so far: {len(state.get('findings', []))}\n",
        "- Searches performed: {state.get('search_count', 0)}/{state.get('max_searches', 5)}\n",
        "- Confidence: {state.get('confidence_score', 0.0):.1f}/10.0\n",
        "\"\"\"\n",
        "    \n",
        "    # Add context to messages for the LLM\n",
        "    full_messages = messages + [HumanMessage(content=context_message)]\n",
        "    response = research_llm.invoke(full_messages)\n",
        "    \n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "def research_tool_node(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Enhanced tool node that updates research state\"\"\"\n",
        "    # Execute tools\n",
        "    tool_node_result = ToolNode(research_tools)(state)\n",
        "    \n",
        "    # Update research-specific state\n",
        "    updates = {}\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            if tool_call['name'] == 'web_search':\n",
        "                # Increment search count\n",
        "                updates[\"search_count\"] = state.get(\"search_count\", 0) + 1\n",
        "                \n",
        "                # Add to findings\n",
        "                current_findings = state.get(\"findings\", [])\n",
        "                query = tool_call['args']['query']\n",
        "                current_findings.append(f\"Search: {query}\")\n",
        "                updates[\"findings\"] = current_findings\n",
        "                \n",
        "                # Update confidence based on number of searches\n",
        "                search_count = updates[\"search_count\"]\n",
        "                updates[\"confidence_score\"] = min(search_count * 2.0, 10.0)\n",
        "    \n",
        "    # Combine tool results with state updates\n",
        "    result = {\"messages\": tool_node_result[\"messages\"]}\n",
        "    result.update(updates)\n",
        "    \n",
        "    return result\n",
        "\n",
        "def initialize_research(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Initialize research session if needed\"\"\"\n",
        "    updates = {}\n",
        "    \n",
        "    if not state.get(\"research_topic\"):\n",
        "        # Extract topic from the first message\n",
        "        if state[\"messages\"]:\n",
        "            first_msg = state[\"messages\"][0].content\n",
        "            updates[\"research_topic\"] = first_msg[:100]  # First 100 chars as topic\n",
        "    \n",
        "    if \"search_count\" not in state:\n",
        "        updates[\"search_count\"] = 0\n",
        "    \n",
        "    if \"findings\" not in state:\n",
        "        updates[\"findings\"] = []\n",
        "        \n",
        "    if \"confidence_score\" not in state:\n",
        "        updates[\"confidence_score\"] = 0.0\n",
        "        \n",
        "    if \"max_searches\" not in state:\n",
        "        updates[\"max_searches\"] = 5\n",
        "    \n",
        "    return updates\n",
        "\n",
        "print(\"‚úÖ Research assistant nodes created!\")\n",
        "print(\"- research_chatbot: Handles conversations with research context\")\n",
        "print(\"- research_tool_node: Executes tools and updates research state\") \n",
        "print(\"- initialize_research: Sets up initial research parameters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build the Research Assistant Graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Advanced research assistant created!\n",
            "Flow: START ‚Üí initialize ‚Üí chatbot ‚Üí [tools/complete/end] ‚Üí ...\n"
          ]
        }
      ],
      "source": [
        "def research_router(state: ResearchState) -> Literal[\"tools\", \"complete\", \"__end__\"]:\n",
        "    \"\"\"Advanced routing logic for research assistant\"\"\"\n",
        "    \n",
        "    # Check if we've reached max searches\n",
        "    if state.get(\"search_count\", 0) >= state.get(\"max_searches\", 5):\n",
        "        return \"complete\"\n",
        "    \n",
        "    # Check for tool calls\n",
        "    messages = state[\"messages\"]\n",
        "    if messages:\n",
        "        last_message = messages[-1]\n",
        "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "            return \"tools\"\n",
        "    \n",
        "    return \"__end__\"\n",
        "\n",
        "def complete_research(state: ResearchState) -> Dict[str, Any]:\n",
        "    \"\"\"Finalize research and provide summary\"\"\"\n",
        "    findings = state.get(\"findings\", [])\n",
        "    confidence = state.get(\"confidence_score\", 0.0)\n",
        "    topic = state.get(\"research_topic\", \"Unknown\")\n",
        "    \n",
        "    summary = f\"\"\"\n",
        "Research Complete!\n",
        "\n",
        "Topic: {topic}\n",
        "Total findings: {len(findings)}\n",
        "Confidence score: {confidence:.1f}/10.0\n",
        "\n",
        "Summary of findings:\n",
        "{chr(10).join(f\"- {finding}\" for finding in findings)}\n",
        "\n",
        "Research session concluded.\n",
        "\"\"\"\n",
        "    \n",
        "    return {\"messages\": [AIMessage(content=summary)]}\n",
        "\n",
        "# Build the research assistant graph\n",
        "research_builder = StateGraph(ResearchState)\n",
        "\n",
        "# Add nodes\n",
        "research_builder.add_node(\"initialize\", initialize_research)\n",
        "research_builder.add_node(\"chatbot\", research_chatbot)\n",
        "research_builder.add_node(\"tools\", research_tool_node)\n",
        "research_builder.add_node(\"complete\", complete_research)\n",
        "\n",
        "# Add edges\n",
        "research_builder.add_edge(START, \"initialize\")\n",
        "research_builder.add_edge(\"initialize\", \"chatbot\")\n",
        "\n",
        "# Add conditional routing\n",
        "research_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    research_router,\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        \"complete\": \"complete\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "research_builder.add_edge(\"tools\", \"chatbot\")\n",
        "research_builder.add_edge(\"complete\", END)\n",
        "\n",
        "# Compile\n",
        "research_assistant = research_builder.compile(checkpointer=memory)\n",
        "\n",
        "print(\"‚úÖ Advanced research assistant created!\")\n",
        "print(\"Flow: START ‚Üí initialize ‚Üí chatbot ‚Üí [tools/complete/end] ‚Üí ...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test the Advanced Research Assistant\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Testing Advanced Research Assistant\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_ncSr0FhW02n2pRt3nmYgG1mP\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Test the research assistant\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m research_result = \u001b[43mtest_research_assistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtest_research_assistant\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m config = {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mresearch_session_1\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Start research\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43mresearch_assistant\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI want to research machine learning applications\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä Research Session Started\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mresearch_chatbot\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Add context to messages for the LLM\u001b[39;00m\n\u001b[32m     15\u001b[39m full_messages = messages + [HumanMessage(content=context_message)]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mresearch_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_messages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [response]}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langchain_core/runnables/base.py:5431\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5424\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5425\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5426\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5429\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5430\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5432\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5433\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5434\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5435\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:776\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    773\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m776\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    780\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    781\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    782\u001b[39m         )\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    784\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:1022\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1020\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1021\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1022\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:973\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    971\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, generation_info)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/openai/_utils/_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/openai/_base_client.py:1239\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1225\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1227\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1234\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1235\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1236\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1237\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1238\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/bia_genai/lib/python3.11/site-packages/openai/_base_client.py:1034\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1031\u001b[39m             err.response.read()\n\u001b[32m   1033\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1034\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_ncSr0FhW02n2pRt3nmYgG1mP\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}",
            "During task with name 'chatbot' and id '6ff82353-9166-f263-5676-8448e2050e99'"
          ]
        }
      ],
      "source": [
        "def test_research_assistant():\n",
        "    print(\"üî¨ Testing Advanced Research Assistant\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"research_session_1\"}}\n",
        "    \n",
        "    # Start research\n",
        "    result = research_assistant.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"I want to research machine learning applications\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    print(\"üìä Research Session Started\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    # Show state after initialization\n",
        "    state = research_assistant.get_state(config)\n",
        "    print(f\"Topic: {state.values.get('research_topic', 'Not set')}\")\n",
        "    print(f\"Search count: {state.values.get('search_count', 0)}\")\n",
        "    print(f\"Confidence: {state.values.get('confidence_score', 0.0):.1f}\")\n",
        "    print(f\"Findings: {len(state.values.get('findings', []))}\")\n",
        "    \n",
        "    # Continue with searches\n",
        "    print(\"\\nüìù Requesting searches...\")\n",
        "    result = research_assistant.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"Please search for information about machine learning\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    # Show updated state\n",
        "    state = research_assistant.get_state(config)\n",
        "    print(f\"\\nAfter search:\")\n",
        "    print(f\"Search count: {state.values.get('search_count', 0)}\")\n",
        "    print(f\"Confidence: {state.values.get('confidence_score', 0.0):.1f}\")\n",
        "    print(f\"Findings: {state.values.get('findings', [])}\")\n",
        "    \n",
        "    # Show final messages\n",
        "    print(\"\\nüí¨ Final Response:\")\n",
        "    for message in result[\"messages\"][-2:]:  # Show last 2 messages\n",
        "        if isinstance(message, HumanMessage):\n",
        "            print(f\"üë§ Human: {message.content}\")\n",
        "        elif isinstance(message, AIMessage):\n",
        "            print(f\"ü§ñ AI: {message.content[:200]}...\")  # Truncate for readability\n",
        "        elif isinstance(message, ToolMessage):\n",
        "            print(f\"üîß Tool: {message.content[:100]}...\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Test the research assistant\n",
        "research_result = test_research_assistant()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üë• Part 7: Multi-Agent Systems {#part7}\n",
        "\n",
        "LangGraph really shines when building systems with multiple specialized agents working together. Let's create a simple multi-agent system with:\n",
        "\n",
        "1. **Researcher Agent**: Gathers information\n",
        "2. **Analyst Agent**: Analyzes the information\n",
        "3. **Writer Agent**: Creates final reports\n",
        "\n",
        "### Multi-Agent State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiAgentState(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "    task: str\n",
        "    research_data: List[str]\n",
        "    analysis_results: List[str]\n",
        "    final_report: str\n",
        "    current_agent: str\n",
        "    task_complete: bool\n",
        "\n",
        "# Specialized agents\n",
        "def researcher_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Specialized research agent\"\"\"\n",
        "    \n",
        "    # Create research-focused prompt\n",
        "    research_prompt = f\"\"\"\n",
        "You are a Research Agent. Your job is to gather information about: {state.get('task', 'the given topic')}.\n",
        "\n",
        "Current research data: {state.get('research_data', [])}\n",
        "\n",
        "Please search for relevant information and add your findings to the research data.\n",
        "Focus on factual information and reliable sources.\n",
        "\"\"\"\n",
        "    \n",
        "    messages = state[\"messages\"] + [HumanMessage(content=research_prompt)]\n",
        "    response = research_llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [response],\n",
        "        \"current_agent\": \"researcher\"\n",
        "    }\n",
        "\n",
        "def analyst_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Specialized analysis agent\"\"\"\n",
        "    \n",
        "    research_data = state.get('research_data', [])\n",
        "    analysis_prompt = f\"\"\"\n",
        "You are an Analysis Agent. Your job is to analyze the research data and provide insights.\n",
        "\n",
        "Research data to analyze:\n",
        "{chr(10).join(f\"- {data}\" for data in research_data)}\n",
        "\n",
        "Please provide analysis, identify patterns, and draw conclusions.\n",
        "Focus on insights and implications.\n",
        "\"\"\"\n",
        "    \n",
        "    messages = state[\"messages\"] + [HumanMessage(content=analysis_prompt)]\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    # Extract analysis from response (simplified)\n",
        "    analysis_results = state.get('analysis_results', [])\n",
        "    analysis_results.append(f\"Analysis: {response.content[:200]}...\")\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [response],\n",
        "        \"analysis_results\": analysis_results,\n",
        "        \"current_agent\": \"analyst\"\n",
        "    }\n",
        "\n",
        "def writer_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Specialized writing agent\"\"\"\n",
        "    \n",
        "    research_data = state.get('research_data', [])\n",
        "    analysis_results = state.get('analysis_results', [])\n",
        "    \n",
        "    writing_prompt = f\"\"\"\n",
        "You are a Writing Agent. Create a comprehensive final report.\n",
        "\n",
        "Research Data:\n",
        "{chr(10).join(f\"- {data}\" for data in research_data)}\n",
        "\n",
        "Analysis Results:\n",
        "{chr(10).join(f\"- {result}\" for result in analysis_results)}\n",
        "\n",
        "Please create a well-structured final report that combines the research and analysis.\n",
        "\"\"\"\n",
        "    \n",
        "    messages = state[\"messages\"] + [HumanMessage(content=writing_prompt)]\n",
        "    response = llm.invoke(messages)\n",
        "    \n",
        "    return {\n",
        "        \"messages\": [response],\n",
        "        \"final_report\": response.content,\n",
        "        \"current_agent\": \"writer\",\n",
        "        \"task_complete\": True\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Multi-agent system agents created!\")\n",
        "print(\"- Researcher Agent: Gathers information\")\n",
        "print(\"- Analyst Agent: Analyzes findings\") \n",
        "print(\"- Writer Agent: Creates final reports\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Multi-Agent Workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multi_agent_router(state: MultiAgentState) -> Literal[\"researcher\", \"analyst\", \"writer\", \"__end__\"]:\n",
        "    \"\"\"Route between different agents based on current state\"\"\"\n",
        "    \n",
        "    if state.get(\"task_complete\", False):\n",
        "        return \"__end__\"\n",
        "    \n",
        "    current_agent = state.get(\"current_agent\", \"\")\n",
        "    research_data = state.get(\"research_data\", [])\n",
        "    analysis_results = state.get(\"analysis_results\", [])\n",
        "    \n",
        "    # Start with researcher\n",
        "    if not current_agent:\n",
        "        return \"researcher\"\n",
        "    \n",
        "    # After researcher, go to analyst if we have research data\n",
        "    if current_agent == \"researcher\" and len(research_data) > 0:\n",
        "        return \"analyst\"\n",
        "    \n",
        "    # After analyst, go to writer if we have analysis\n",
        "    if current_agent == \"analyst\" and len(analysis_results) > 0:\n",
        "        return \"writer\"\n",
        "    \n",
        "    # Continue with current agent if more work needed\n",
        "    if current_agent == \"researcher\" and len(research_data) < 2:\n",
        "        return \"researcher\"\n",
        "    \n",
        "    return \"__end__\"\n",
        "\n",
        "def initialize_multi_agent(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Initialize multi-agent state\"\"\"\n",
        "    updates = {}\n",
        "    \n",
        "    if not state.get(\"task\") and state.get(\"messages\"):\n",
        "        # Extract task from first message\n",
        "        first_msg = state[\"messages\"][0].content\n",
        "        updates[\"task\"] = first_msg\n",
        "    \n",
        "    if \"research_data\" not in state:\n",
        "        updates[\"research_data\"] = []\n",
        "    \n",
        "    if \"analysis_results\" not in state:\n",
        "        updates[\"analysis_results\"] = []\n",
        "    \n",
        "    if \"current_agent\" not in state:\n",
        "        updates[\"current_agent\"] = \"\"\n",
        "    \n",
        "    if \"task_complete\" not in state:\n",
        "        updates[\"task_complete\"] = False\n",
        "    \n",
        "    return updates\n",
        "\n",
        "# Enhanced tool node for multi-agent\n",
        "def multi_agent_tool_node(state: MultiAgentState) -> Dict[str, Any]:\n",
        "    \"\"\"Tool node that updates research data\"\"\"\n",
        "    tool_result = ToolNode(research_tools)(state)\n",
        "    \n",
        "    # Update research data if web search was performed\n",
        "    updates = {}\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    \n",
        "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
        "        for tool_call in last_message.tool_calls:\n",
        "            if tool_call['name'] == 'web_search':\n",
        "                research_data = state.get(\"research_data\", [])\n",
        "                query = tool_call['args']['query']\n",
        "                # Add the search result to research data\n",
        "                research_data.append(f\"Search result for '{query}': {tool_result['messages'][-1].content}\")\n",
        "                updates[\"research_data\"] = research_data\n",
        "    \n",
        "    result = {\"messages\": tool_result[\"messages\"]}\n",
        "    result.update(updates)\n",
        "    return result\n",
        "\n",
        "# Build multi-agent graph\n",
        "multi_agent_builder = StateGraph(MultiAgentState)\n",
        "\n",
        "# Add nodes\n",
        "multi_agent_builder.add_node(\"initialize\", initialize_multi_agent)\n",
        "multi_agent_builder.add_node(\"researcher\", researcher_agent)\n",
        "multi_agent_builder.add_node(\"tools\", multi_agent_tool_node)\n",
        "multi_agent_builder.add_node(\"analyst\", analyst_agent)\n",
        "multi_agent_builder.add_node(\"writer\", writer_agent)\n",
        "\n",
        "# Add edges\n",
        "multi_agent_builder.add_edge(START, \"initialize\")\n",
        "\n",
        "# From initialize, route to appropriate agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"initialize\",\n",
        "    multi_agent_router,\n",
        "    {\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Researcher can use tools or continue to next agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"researcher\",\n",
        "    lambda state: \"tools\" if (\n",
        "        state[\"messages\"] and \n",
        "        hasattr(state[\"messages\"][-1], 'tool_calls') and \n",
        "        state[\"messages\"][-1].tool_calls\n",
        "    ) else multi_agent_router(state),\n",
        "    {\n",
        "        \"tools\": \"tools\",\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# After tools, route to next agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"tools\",\n",
        "    multi_agent_router,\n",
        "    {\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Analyst routes to next agent\n",
        "multi_agent_builder.add_conditional_edges(\n",
        "    \"analyst\",\n",
        "    multi_agent_router,\n",
        "    {\n",
        "        \"researcher\": \"researcher\",\n",
        "        \"analyst\": \"analyst\",\n",
        "        \"writer\": \"writer\",\n",
        "        \"__end__\": END\n",
        "    }\n",
        ")\n",
        "\n",
        "# Writer completes the task\n",
        "multi_agent_builder.add_edge(\"writer\", END)\n",
        "\n",
        "# Compile\n",
        "multi_agent_system = multi_agent_builder.compile(checkpointer=checkpointer)\n",
        "\n",
        "print(\"‚úÖ Multi-agent system created!\")\n",
        "print(\"Flow: initialize ‚Üí researcher ‚Üí [tools] ‚Üí analyst ‚Üí writer ‚Üí END\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Test Multi-Agent System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_multi_agent_system():\n",
        "    print(\"üë• Testing Multi-Agent System\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    config = {\"configurable\": {\"thread_id\": \"multi_agent_test\"}}\n",
        "    \n",
        "    # Start the multi-agent workflow\n",
        "    result = multi_agent_system.invoke(\n",
        "        {\"messages\": [HumanMessage(content=\"Research and analyze the benefits of renewable energy\")]},\n",
        "        config\n",
        "    )\n",
        "    \n",
        "    # Get final state\n",
        "    final_state = multi_agent_system.get_state(config)\n",
        "    \n",
        "    print(\"üìä Multi-Agent Workflow Complete!\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"Task: {final_state.values.get('task', 'Not set')}\")\n",
        "    print(f\"Current Agent: {final_state.values.get('current_agent', 'None')}\")\n",
        "    print(f\"Task Complete: {final_state.values.get('task_complete', False)}\")\n",
        "    \n",
        "    print(f\"\\nüî¨ Research Data ({len(final_state.values.get('research_data', []))}):\")\n",
        "    for i, data in enumerate(final_state.values.get('research_data', []), 1):\n",
        "        print(f\"{i}. {data[:100]}...\")\n",
        "    \n",
        "    print(f\"\\nüìà Analysis Results ({len(final_state.values.get('analysis_results', []))}):\")\n",
        "    for i, analysis in enumerate(final_state.values.get('analysis_results', []), 1):\n",
        "        print(f\"{i}. {analysis[:100]}...\")\n",
        "    \n",
        "    print(f\"\\nüìù Final Report:\")\n",
        "    final_report = final_state.values.get('final_report', 'No report generated')\n",
        "    print(final_report[:300] + \"...\" if len(final_report) > 300 else final_report)\n",
        "    \n",
        "    print(f\"\\nüí¨ Total Messages: {len(final_state.values.get('messages', []))}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Test the multi-agent system\n",
        "multi_agent_result = test_multi_agent_system()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üåü Part 8: Real-World Use Cases {#part8}\n",
        "\n",
        "Now that you understand LangGraph fundamentals, let's explore some real-world applications:\n",
        "\n",
        "### üéØ Use Case Examples\n",
        "\n",
        "1. **Customer Support Agent**\n",
        "   - Handles inquiries with memory\n",
        "   - Escalates to humans when needed\n",
        "   - Accesses knowledge bases and APIs\n",
        "\n",
        "2. **Code Review Assistant**  \n",
        "   - Analyzes code for issues\n",
        "   - Suggests improvements\n",
        "   - Runs tests and checks\n",
        "\n",
        "3. **Content Creation Pipeline**\n",
        "   - Research ‚Üí Outline ‚Üí Write ‚Üí Edit ‚Üí Publish\n",
        "   - Multiple specialized agents\n",
        "   - Human approval at key stages\n",
        "\n",
        "4. **Data Analysis Workflow**\n",
        "   - Data ingestion ‚Üí Cleaning ‚Üí Analysis ‚Üí Visualization ‚Üí Report\n",
        "\n",
        "### üí° Key Patterns You've Learned\n",
        "\n",
        "1. **State Management**: Custom state for complex workflows\n",
        "2. **Tool Integration**: External APIs and functions\n",
        "3. **Human-in-the-Loop**: Interrupts and approvals\n",
        "4. **Multi-Agent Coordination**: Specialized agents working together\n",
        "5. **Conditional Routing**: Dynamic decision making\n",
        "6. **Memory & Persistence**: Conversation continuity\n",
        "\n",
        "### üöÄ Next Steps\n",
        "\n",
        "To build production applications:\n",
        "\n",
        "1. **Error Handling**: Add try-catch blocks and error recovery\n",
        "2. **Authentication**: Secure your APIs and data\n",
        "3. **Monitoring**: Use LangSmith for observability\n",
        "4. **Scaling**: Consider deployment options\n",
        "5. **Testing**: Write comprehensive tests\n",
        "6. **Documentation**: Document your workflows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéì Conclusion & Summary\n",
        "\n",
        "Congratulations! üéâ You've completed the comprehensive LangGraph tutorial. Here's what you've accomplished:\n",
        "\n",
        "### ‚úÖ What You've Built\n",
        "\n",
        "1. **Simple Chatbot** - Basic conversation with LLM\n",
        "2. **Memory-Enabled Agent** - Persistent conversations with checkpointing  \n",
        "3. **Tool-Integrated Agent** - External API calls and function execution\n",
        "4. **Human-in-the-Loop System** - Interrupts and human oversight\n",
        "5. **Advanced State Management** - Complex workflows with custom state\n",
        "6. **Multi-Agent System** - Coordinated specialized agents\n",
        "\n",
        "### üß† Core Concepts Mastered\n",
        "\n",
        "- **State**: The memory and data of your application\n",
        "- **Nodes**: Functions that process and transform state\n",
        "- **Edges**: Connections that define workflow flow\n",
        "- **Conditional Routing**: Dynamic decision making\n",
        "- **Checkpointing**: Persistent memory across sessions\n",
        "- **Interrupts**: Human-in-the-loop capabilities\n",
        "- **Tools**: External function integration\n",
        "\n",
        "### üìö Additional Resources\n",
        "\n",
        "- **Official Documentation**: https://langchain-ai.github.io/langgraph/\n",
        "- **LangSmith Observability**: https://langsmith.langchain.com/\n",
        "- **LangChain Community**: https://github.com/langchain-ai/langchain\n",
        "- **Examples Repository**: https://github.com/langchain-ai/langgraph/tree/main/examples\n",
        "\n",
        "### üöÄ Your Next Journey\n",
        "\n",
        "You now have the foundation to build sophisticated AI agents and workflows. Start with a simple use case and gradually add complexity as you become more comfortable with the patterns.\n",
        "\n",
        "**Happy building with LangGraph!** üåü\n",
        "\n",
        "---\n",
        "\n",
        "*This tutorial was created based on the official LangGraph documentation and best practices. For the latest updates and features, always refer to the official documentation.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Complete LangGraph Tutorial: From Basics to Advanced Applications\n",
        "\n",
        "## üìñ Comprehensive Guide for Building Stateful AI Agents\n",
        "\n",
        "Welcome to the most comprehensive LangGraph tutorial! This notebook will take you from complete beginner to building sophisticated AI agents step by step.\n",
        "\n",
        "### üéØ What You'll Learn\n",
        "\n",
        "By the end of this tutorial, you will:\n",
        "- ‚úÖ Understand LangGraph's core concepts (Nodes, Edges, State)\n",
        "- ‚úÖ Build your first simple chatbot\n",
        "- ‚úÖ Add memory and persistence to conversations\n",
        "- ‚úÖ Implement tool calling and external integrations\n",
        "- ‚úÖ Create human-in-the-loop workflows\n",
        "- ‚úÖ Build multi-agent systems\n",
        "- ‚úÖ Handle complex state management\n",
        "- ‚úÖ Deploy production-ready applications\n",
        "\n",
        "### üìö Based on Official LangGraph Documentation\n",
        "\n",
        "This tutorial follows the [official LangGraph documentation](https://langchain-ai.github.io/langgraph/) and incorporates best practices from the LangGraph team.\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Table of Contents\n",
        "\n",
        "1. **[Setup & Installation](#setup)**\n",
        "2. **[Part 1: Understanding LangGraph Fundamentals](#part1)**\n",
        "3. **[Part 2: Building Your First Simple Agent](#part2)**\n",
        "4. **[Part 3: Adding Memory with Checkpointing](#part3)**\n",
        "5. **[Part 4: Tool Integration & External APIs](#part4)**\n",
        "6. **[Part 5: Human-in-the-Loop Workflows](#part5)**\n",
        "7. **[Part 6: Advanced State Management](#part6)**\n",
        "8. **[Part 7: Multi-Agent Systems](#part7)**\n",
        "9. **[Part 8: Real-World Use Cases](#part8)**\n",
        "10. **[Part 9: Production Deployment](#part9)**\n",
        "\n",
        "Let's begin this exciting journey! üåü\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "bia_genai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
